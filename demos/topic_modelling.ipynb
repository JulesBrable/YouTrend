{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Yedidia\n",
      "[nltk_data]     AGNIMO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Yedidia\n",
      "[nltk_data]     AGNIMO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Remove stop words (and word with less than 3 letters, example \"eat\" no-sense, to be check later)\n",
    "- [ ] Lemmatization (transform words into their radical form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../assets/data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20972, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1        Reconstructing Subject-Specific Effect Maps   \n",
       "1   2                 Rotation Invariance Neural Network   \n",
       "2   3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3   4  A finite element approximation for the stochas...   \n",
       "4   5  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                                            ABSTRACT  Computer Science  \\\n",
       "0    Predictive models allow subject-specific inf...                 1   \n",
       "1    Rotation invariance and translation invarian...                 1   \n",
       "2    We introduce and develop the notion of spher...                 0   \n",
       "3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
       "4    Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
       "\n",
       "   Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0        0            0           0                     0   \n",
       "1        0            0           0                     0   \n",
       "2        0            1           0                     0   \n",
       "3        0            1           0                     0   \n",
       "4        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "  return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='n'))\n",
    "\n",
    "\n",
    "# Delete stopwords\n",
    "def preprocess(text) :\n",
    "  result = []\n",
    "  for token in gensim.utils.simple_preprocess(text) :\n",
    "    if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 :\n",
    "      result.append(lemmatize_stemming(token))\n",
    "  return result\n",
    "\n",
    "# Extracting lemma of each word in each \"document\" (asbtract of research paper)\n",
    "processed_docs = [preprocess(doc) for doc in data[\"ABSTRACT\"][:200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 200\n"
     ]
    }
   ],
   "source": [
    "print(type(processed_docs), len(processed_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data.\n",
    "\n",
    "* Data is stokcked into a `gensim`'s dictionary, then convert into **Bag of Words**. That is in couples (\"word\": occurence_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1d1027d7c70>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dictionary.\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "# tokens in at least `no_below` documents, in at most (`no_above` * nb_documents) documents\n",
    "# keep only the `keep_n` most frequent tokens\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=50)\n",
    "\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document frequencies: {8: 18, 11: 20, 1: 15, 3: 20, 7: 17, 9: 17, 10: 20, 5: 17, 2: 15, 12: 18, 4: 16, 0: 18, 6: 16, 17: 20, 16: 20, 14: 16, 15: 15, 13: 19, 19: 20, 21: 17, 20: 18, 18: 20, 23: 18, 26: 19, 24: 15, 22: 15, 25: 20, 27: 17, 29: 20, 31: 17, 30: 15, 28: 16, 33: 16, 32: 16, 34: 15, 36: 16, 35: 18, 37: 17, 39: 18, 40: 20, 38: 16, 41: 17, 43: 19, 45: 16, 42: 16, 44: 19, 46: 16, 49: 16, 48: 15, 47: 18, 50: 15, 51: 16, 52: 20, 53: 17, 54: 16, 55: 19, 56: 19, 58: 20, 57: 16, 59: 16, 60: 16, 61: 18, 62: 15, 63: 16, 64: 19, 65: 15, 66: 20, 67: 17}\n",
      "Number of documents processed: 200\n",
      "Number of corpus position (number of processed words): 15871\n",
      "Number of non-zeroes in BOW matrix (sum of unique words per document): 11187\n",
      "Dictionary {'dataset': 0, 'detect': 1, 'exampl': 2, 'exist': 3, 'experiment': 4, 'finit': 5, 'imag': 6, 'improv': 7, 'predict': 8, 'reduc': 9, 'sampl': 10, 'specif': 11, 'type': 12, 'multipl': 13, 'neural': 14, 'posit': 15, 'task': 16, 'valu': 17, 'approxim': 18, 'equat': 19, 'novel': 20, 'object': 21, 'appli': 22, 'featur': 23, 'show': 24, 'signific': 25, 'techniqu': 26, 'bound': 27, 'final': 28, 'point': 29, 'possibl': 30, 'set': 31, 'chang': 32, 'variabl': 33, 'control': 34, 'establish': 35, 'surfac': 36, 'investig': 37, 'avail': 38, 'known': 39, 'long': 40, 'standard': 41, 'calcul': 42, 'import': 43, 'oper': 44, 'simpl': 45, 'interact': 46, 'mean': 47, 'need': 48, 'special': 49, 'correspond': 50, 'strong': 51, 'class': 52, 'previous': 53, 'group': 54, 'lead': 55, 'system': 56, 'rang': 57, 'real': 58, 'random': 59, 'implement': 60, 'solv': 61, 'electron': 62, 'identifi': 63, 'requir': 64, 'power': 65, 'framework': 66, 'challeng': 67}\n"
     ]
    }
   ],
   "source": [
    "print(\"Document frequencies:\", dictionary.dfs)\n",
    "print(\"Number of documents processed:\", dictionary.num_docs)\n",
    "print(\"Number of corpus position (number of processed words):\", dictionary.num_pos)\n",
    "print(\"Number of non-zeroes in BOW matrix (sum of unique words per document):\", dictionary.num_nnz)\n",
    "print(\"Dictionary\", dictionary.token2id)\n",
    "\n",
    "# print(dictionary.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (1, 6), (2, 1), (3, 1), (4, 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Words\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA model. THe number of topics per document has to be determined.\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 20, id2word = dictionary, passes = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 -> Words: 0.223*\"identifi\" + 0.169*\"implement\" + 0.113*\"standard\" + 0.071*\"need\" + 0.057*\"signific\" + 0.055*\"oper\" + 0.044*\"avail\" + 0.043*\"appli\" + 0.039*\"rang\" + 0.034*\"framework\"\n",
      "Topic: 1 -> Words: 0.243*\"specif\" + 0.146*\"multipl\" + 0.100*\"neural\" + 0.079*\"valu\" + 0.052*\"set\" + 0.051*\"exampl\" + 0.049*\"type\" + 0.045*\"posit\" + 0.034*\"establish\" + 0.031*\"techniqu\"\n",
      "Topic: 2 -> Words: 0.262*\"detect\" + 0.163*\"system\" + 0.062*\"task\" + 0.041*\"signific\" + 0.041*\"interact\" + 0.041*\"control\" + 0.041*\"avail\" + 0.041*\"oper\" + 0.041*\"investig\" + 0.023*\"real\"\n",
      "Topic: 3 -> Words: 0.153*\"interact\" + 0.122*\"point\" + 0.108*\"finit\" + 0.089*\"bound\" + 0.067*\"equat\" + 0.057*\"lead\" + 0.045*\"special\" + 0.045*\"implement\" + 0.040*\"type\" + 0.027*\"appli\"\n",
      "Topic: 4 -> Words: 0.316*\"class\" + 0.126*\"posit\" + 0.092*\"correspond\" + 0.072*\"set\" + 0.065*\"random\" + 0.055*\"group\" + 0.052*\"finit\" + 0.032*\"multipl\" + 0.027*\"exampl\" + 0.020*\"identifi\"\n",
      "Topic: 5 -> Words: 0.168*\"group\" + 0.138*\"reduc\" + 0.080*\"possibl\" + 0.073*\"improv\" + 0.063*\"exampl\" + 0.057*\"surfac\" + 0.049*\"neural\" + 0.040*\"requir\" + 0.038*\"object\" + 0.037*\"need\"\n",
      "Topic: 6 -> Words: 0.165*\"object\" + 0.122*\"neural\" + 0.115*\"task\" + 0.110*\"dataset\" + 0.078*\"novel\" + 0.073*\"import\" + 0.062*\"featur\" + 0.049*\"predict\" + 0.039*\"detect\" + 0.034*\"signific\"\n",
      "Topic: 7 -> Words: 0.471*\"power\" + 0.085*\"previous\" + 0.068*\"type\" + 0.067*\"chang\" + 0.053*\"requir\" + 0.051*\"long\" + 0.047*\"standard\" + 0.035*\"identifi\" + 0.021*\"simpl\" + 0.020*\"posit\"\n",
      "Topic: 8 -> Words: 0.243*\"approxim\" + 0.141*\"featur\" + 0.095*\"task\" + 0.088*\"control\" + 0.080*\"improv\" + 0.036*\"signific\" + 0.036*\"standard\" + 0.036*\"strong\" + 0.036*\"lead\" + 0.036*\"need\"\n",
      "Topic: 9 -> Words: 0.139*\"challeng\" + 0.114*\"investig\" + 0.094*\"object\" + 0.081*\"predict\" + 0.081*\"long\" + 0.067*\"oper\" + 0.064*\"real\" + 0.052*\"task\" + 0.049*\"improv\" + 0.033*\"rang\"\n",
      "Topic: 10 -> Words: 0.224*\"bound\" + 0.170*\"simpl\" + 0.165*\"type\" + 0.087*\"investig\" + 0.045*\"show\" + 0.035*\"exist\" + 0.035*\"framework\" + 0.032*\"strong\" + 0.022*\"valu\" + 0.021*\"solv\"\n",
      "Topic: 11 -> Words: 0.221*\"predict\" + 0.132*\"strong\" + 0.122*\"electron\" + 0.113*\"sampl\" + 0.066*\"finit\" + 0.061*\"experiment\" + 0.032*\"point\" + 0.032*\"multipl\" + 0.032*\"control\" + 0.022*\"mean\"\n",
      "Topic: 12 -> Words: 0.282*\"known\" + 0.134*\"requir\" + 0.123*\"standard\" + 0.118*\"avail\" + 0.113*\"techniqu\" + 0.042*\"long\" + 0.041*\"solv\" + 0.021*\"reduc\" + 0.019*\"task\" + 0.019*\"previous\"\n",
      "Topic: 13 -> Words: 0.155*\"calcul\" + 0.123*\"establish\" + 0.111*\"random\" + 0.050*\"surfac\" + 0.050*\"approxim\" + 0.050*\"mean\" + 0.044*\"group\" + 0.040*\"electron\" + 0.037*\"strong\" + 0.037*\"featur\"\n",
      "Topic: 14 -> Words: 0.106*\"valu\" + 0.078*\"special\" + 0.069*\"appli\" + 0.062*\"known\" + 0.062*\"show\" + 0.058*\"import\" + 0.049*\"oper\" + 0.046*\"correspond\" + 0.046*\"mean\" + 0.042*\"long\"\n",
      "Topic: 15 -> Words: 0.219*\"electron\" + 0.191*\"point\" + 0.074*\"featur\" + 0.069*\"valu\" + 0.053*\"exist\" + 0.045*\"calcul\" + 0.041*\"investig\" + 0.030*\"specif\" + 0.030*\"lead\" + 0.028*\"signific\"\n",
      "Topic: 16 -> Words: 0.184*\"sampl\" + 0.128*\"imag\" + 0.113*\"detect\" + 0.070*\"approxim\" + 0.066*\"dataset\" + 0.044*\"solv\" + 0.037*\"real\" + 0.034*\"set\" + 0.033*\"improv\" + 0.032*\"bound\"\n",
      "Topic: 17 -> Words: 0.337*\"system\" + 0.117*\"framework\" + 0.074*\"real\" + 0.064*\"avail\" + 0.044*\"challeng\" + 0.033*\"solv\" + 0.032*\"need\" + 0.025*\"show\" + 0.025*\"oper\" + 0.024*\"possibl\"\n",
      "Topic: 18 -> Words: 0.256*\"equat\" + 0.116*\"surfac\" + 0.107*\"lead\" + 0.072*\"final\" + 0.054*\"experiment\" + 0.051*\"rang\" + 0.042*\"long\" + 0.037*\"calcul\" + 0.037*\"investig\" + 0.037*\"real\"\n",
      "Topic: 19 -> Words: 0.146*\"chang\" + 0.142*\"variabl\" + 0.138*\"control\" + 0.064*\"exist\" + 0.057*\"solv\" + 0.049*\"import\" + 0.046*\"framework\" + 0.046*\"novel\" + 0.036*\"calcul\" + 0.035*\"valu\"\n"
     ]
    }
   ],
   "source": [
    "topics = []\n",
    "for idx, topic in lda_model.print_topics(-1) :\n",
    "    print(\"Topic: {} -> Words: {}\".format(idx, topic))\n",
    "    topics.append(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.24403910529390135\n"
     ]
    }
   ],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary)\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>(0.223, identifi)</td>\n",
       "      <td>(0.169, implement)</td>\n",
       "      <td>(0.113, standard)</td>\n",
       "      <td>(0.071, need)</td>\n",
       "      <td>(0.057, signific)</td>\n",
       "      <td>(0.055, oper)</td>\n",
       "      <td>(0.044, avail)</td>\n",
       "      <td>(0.043, appli)</td>\n",
       "      <td>(0.039, rang)</td>\n",
       "      <td>(0.034, framework)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>(0.243, specif)</td>\n",
       "      <td>(0.146, multipl)</td>\n",
       "      <td>(0.100, neural)</td>\n",
       "      <td>(0.079, valu)</td>\n",
       "      <td>(0.052, set)</td>\n",
       "      <td>(0.051, exampl)</td>\n",
       "      <td>(0.049, type)</td>\n",
       "      <td>(0.045, posit)</td>\n",
       "      <td>(0.034, establish)</td>\n",
       "      <td>(0.031, techniqu)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>(0.262, detect)</td>\n",
       "      <td>(0.163, system)</td>\n",
       "      <td>(0.062, task)</td>\n",
       "      <td>(0.041, signific)</td>\n",
       "      <td>(0.041, interact)</td>\n",
       "      <td>(0.041, control)</td>\n",
       "      <td>(0.041, avail)</td>\n",
       "      <td>(0.041, oper)</td>\n",
       "      <td>(0.041, investig)</td>\n",
       "      <td>(0.023, real)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>(0.153, interact)</td>\n",
       "      <td>(0.122, point)</td>\n",
       "      <td>(0.108, finit)</td>\n",
       "      <td>(0.089, bound)</td>\n",
       "      <td>(0.067, equat)</td>\n",
       "      <td>(0.057, lead)</td>\n",
       "      <td>(0.045, special)</td>\n",
       "      <td>(0.045, implement)</td>\n",
       "      <td>(0.040, type)</td>\n",
       "      <td>(0.027, appli)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>(0.316, class)</td>\n",
       "      <td>(0.126, posit)</td>\n",
       "      <td>(0.092, correspond)</td>\n",
       "      <td>(0.072, set)</td>\n",
       "      <td>(0.065, random)</td>\n",
       "      <td>(0.055, group)</td>\n",
       "      <td>(0.052, finit)</td>\n",
       "      <td>(0.032, multipl)</td>\n",
       "      <td>(0.027, exampl)</td>\n",
       "      <td>(0.020, identifi)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>(0.168, group)</td>\n",
       "      <td>(0.138, reduc)</td>\n",
       "      <td>(0.080, possibl)</td>\n",
       "      <td>(0.073, improv)</td>\n",
       "      <td>(0.063, exampl)</td>\n",
       "      <td>(0.057, surfac)</td>\n",
       "      <td>(0.049, neural)</td>\n",
       "      <td>(0.040, requir)</td>\n",
       "      <td>(0.038, object)</td>\n",
       "      <td>(0.037, need)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>(0.165, object)</td>\n",
       "      <td>(0.122, neural)</td>\n",
       "      <td>(0.115, task)</td>\n",
       "      <td>(0.110, dataset)</td>\n",
       "      <td>(0.078, novel)</td>\n",
       "      <td>(0.073, import)</td>\n",
       "      <td>(0.062, featur)</td>\n",
       "      <td>(0.049, predict)</td>\n",
       "      <td>(0.039, detect)</td>\n",
       "      <td>(0.034, signific)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>(0.471, power)</td>\n",
       "      <td>(0.085, previous)</td>\n",
       "      <td>(0.068, type)</td>\n",
       "      <td>(0.067, chang)</td>\n",
       "      <td>(0.053, requir)</td>\n",
       "      <td>(0.051, long)</td>\n",
       "      <td>(0.047, standard)</td>\n",
       "      <td>(0.035, identifi)</td>\n",
       "      <td>(0.021, simpl)</td>\n",
       "      <td>(0.020, posit)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>(0.243, approxim)</td>\n",
       "      <td>(0.141, featur)</td>\n",
       "      <td>(0.095, task)</td>\n",
       "      <td>(0.088, control)</td>\n",
       "      <td>(0.080, improv)</td>\n",
       "      <td>(0.036, signific)</td>\n",
       "      <td>(0.036, standard)</td>\n",
       "      <td>(0.036, strong)</td>\n",
       "      <td>(0.036, lead)</td>\n",
       "      <td>(0.036, need)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>(0.139, challeng)</td>\n",
       "      <td>(0.114, investig)</td>\n",
       "      <td>(0.094, object)</td>\n",
       "      <td>(0.081, predict)</td>\n",
       "      <td>(0.081, long)</td>\n",
       "      <td>(0.067, oper)</td>\n",
       "      <td>(0.064, real)</td>\n",
       "      <td>(0.052, task)</td>\n",
       "      <td>(0.049, improv)</td>\n",
       "      <td>(0.033, rang)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 10</th>\n",
       "      <td>(0.224, bound)</td>\n",
       "      <td>(0.170, simpl)</td>\n",
       "      <td>(0.165, type)</td>\n",
       "      <td>(0.087, investig)</td>\n",
       "      <td>(0.045, show)</td>\n",
       "      <td>(0.035, exist)</td>\n",
       "      <td>(0.035, framework)</td>\n",
       "      <td>(0.032, strong)</td>\n",
       "      <td>(0.022, valu)</td>\n",
       "      <td>(0.021, solv)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 11</th>\n",
       "      <td>(0.221, predict)</td>\n",
       "      <td>(0.132, strong)</td>\n",
       "      <td>(0.122, electron)</td>\n",
       "      <td>(0.113, sampl)</td>\n",
       "      <td>(0.066, finit)</td>\n",
       "      <td>(0.061, experiment)</td>\n",
       "      <td>(0.032, point)</td>\n",
       "      <td>(0.032, multipl)</td>\n",
       "      <td>(0.032, control)</td>\n",
       "      <td>(0.022, mean)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 12</th>\n",
       "      <td>(0.282, known)</td>\n",
       "      <td>(0.134, requir)</td>\n",
       "      <td>(0.123, standard)</td>\n",
       "      <td>(0.118, avail)</td>\n",
       "      <td>(0.113, techniqu)</td>\n",
       "      <td>(0.042, long)</td>\n",
       "      <td>(0.041, solv)</td>\n",
       "      <td>(0.021, reduc)</td>\n",
       "      <td>(0.019, task)</td>\n",
       "      <td>(0.019, previous)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 13</th>\n",
       "      <td>(0.155, calcul)</td>\n",
       "      <td>(0.123, establish)</td>\n",
       "      <td>(0.111, random)</td>\n",
       "      <td>(0.050, surfac)</td>\n",
       "      <td>(0.050, approxim)</td>\n",
       "      <td>(0.050, mean)</td>\n",
       "      <td>(0.044, group)</td>\n",
       "      <td>(0.040, electron)</td>\n",
       "      <td>(0.037, strong)</td>\n",
       "      <td>(0.037, featur)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 14</th>\n",
       "      <td>(0.106, valu)</td>\n",
       "      <td>(0.078, special)</td>\n",
       "      <td>(0.069, appli)</td>\n",
       "      <td>(0.062, known)</td>\n",
       "      <td>(0.062, show)</td>\n",
       "      <td>(0.058, import)</td>\n",
       "      <td>(0.049, oper)</td>\n",
       "      <td>(0.046, correspond)</td>\n",
       "      <td>(0.046, mean)</td>\n",
       "      <td>(0.042, long)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 15</th>\n",
       "      <td>(0.219, electron)</td>\n",
       "      <td>(0.191, point)</td>\n",
       "      <td>(0.074, featur)</td>\n",
       "      <td>(0.069, valu)</td>\n",
       "      <td>(0.053, exist)</td>\n",
       "      <td>(0.045, calcul)</td>\n",
       "      <td>(0.041, investig)</td>\n",
       "      <td>(0.030, specif)</td>\n",
       "      <td>(0.030, lead)</td>\n",
       "      <td>(0.028, signific)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 16</th>\n",
       "      <td>(0.184, sampl)</td>\n",
       "      <td>(0.128, imag)</td>\n",
       "      <td>(0.113, detect)</td>\n",
       "      <td>(0.070, approxim)</td>\n",
       "      <td>(0.066, dataset)</td>\n",
       "      <td>(0.044, solv)</td>\n",
       "      <td>(0.037, real)</td>\n",
       "      <td>(0.034, set)</td>\n",
       "      <td>(0.033, improv)</td>\n",
       "      <td>(0.032, bound)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 17</th>\n",
       "      <td>(0.337, system)</td>\n",
       "      <td>(0.117, framework)</td>\n",
       "      <td>(0.074, real)</td>\n",
       "      <td>(0.064, avail)</td>\n",
       "      <td>(0.044, challeng)</td>\n",
       "      <td>(0.033, solv)</td>\n",
       "      <td>(0.032, need)</td>\n",
       "      <td>(0.025, show)</td>\n",
       "      <td>(0.025, oper)</td>\n",
       "      <td>(0.024, possibl)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 18</th>\n",
       "      <td>(0.256, equat)</td>\n",
       "      <td>(0.116, surfac)</td>\n",
       "      <td>(0.107, lead)</td>\n",
       "      <td>(0.072, final)</td>\n",
       "      <td>(0.054, experiment)</td>\n",
       "      <td>(0.051, rang)</td>\n",
       "      <td>(0.042, long)</td>\n",
       "      <td>(0.037, calcul)</td>\n",
       "      <td>(0.037, investig)</td>\n",
       "      <td>(0.037, real)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 19</th>\n",
       "      <td>(0.146, chang)</td>\n",
       "      <td>(0.142, variabl)</td>\n",
       "      <td>(0.138, control)</td>\n",
       "      <td>(0.064, exist)</td>\n",
       "      <td>(0.057, solv)</td>\n",
       "      <td>(0.049, import)</td>\n",
       "      <td>(0.046, framework)</td>\n",
       "      <td>(0.046, novel)</td>\n",
       "      <td>(0.036, calcul)</td>\n",
       "      <td>(0.035, valu)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0                   1                    2  \\\n",
       "Topic 0   (0.223, identifi)  (0.169, implement)    (0.113, standard)   \n",
       "Topic 1     (0.243, specif)    (0.146, multipl)      (0.100, neural)   \n",
       "Topic 2     (0.262, detect)     (0.163, system)        (0.062, task)   \n",
       "Topic 3   (0.153, interact)      (0.122, point)       (0.108, finit)   \n",
       "Topic 4      (0.316, class)      (0.126, posit)  (0.092, correspond)   \n",
       "Topic 5      (0.168, group)      (0.138, reduc)     (0.080, possibl)   \n",
       "Topic 6     (0.165, object)     (0.122, neural)        (0.115, task)   \n",
       "Topic 7      (0.471, power)   (0.085, previous)        (0.068, type)   \n",
       "Topic 8   (0.243, approxim)     (0.141, featur)        (0.095, task)   \n",
       "Topic 9   (0.139, challeng)   (0.114, investig)      (0.094, object)   \n",
       "Topic 10     (0.224, bound)      (0.170, simpl)        (0.165, type)   \n",
       "Topic 11   (0.221, predict)     (0.132, strong)    (0.122, electron)   \n",
       "Topic 12     (0.282, known)     (0.134, requir)    (0.123, standard)   \n",
       "Topic 13    (0.155, calcul)  (0.123, establish)      (0.111, random)   \n",
       "Topic 14      (0.106, valu)    (0.078, special)       (0.069, appli)   \n",
       "Topic 15  (0.219, electron)      (0.191, point)      (0.074, featur)   \n",
       "Topic 16     (0.184, sampl)       (0.128, imag)      (0.113, detect)   \n",
       "Topic 17    (0.337, system)  (0.117, framework)        (0.074, real)   \n",
       "Topic 18     (0.256, equat)     (0.116, surfac)        (0.107, lead)   \n",
       "Topic 19     (0.146, chang)    (0.142, variabl)     (0.138, control)   \n",
       "\n",
       "                          3                    4                    5  \\\n",
       "Topic 0       (0.071, need)    (0.057, signific)        (0.055, oper)   \n",
       "Topic 1       (0.079, valu)         (0.052, set)      (0.051, exampl)   \n",
       "Topic 2   (0.041, signific)    (0.041, interact)     (0.041, control)   \n",
       "Topic 3      (0.089, bound)       (0.067, equat)        (0.057, lead)   \n",
       "Topic 4        (0.072, set)      (0.065, random)       (0.055, group)   \n",
       "Topic 5     (0.073, improv)      (0.063, exampl)      (0.057, surfac)   \n",
       "Topic 6    (0.110, dataset)       (0.078, novel)      (0.073, import)   \n",
       "Topic 7      (0.067, chang)      (0.053, requir)        (0.051, long)   \n",
       "Topic 8    (0.088, control)      (0.080, improv)    (0.036, signific)   \n",
       "Topic 9    (0.081, predict)        (0.081, long)        (0.067, oper)   \n",
       "Topic 10  (0.087, investig)        (0.045, show)       (0.035, exist)   \n",
       "Topic 11     (0.113, sampl)       (0.066, finit)  (0.061, experiment)   \n",
       "Topic 12     (0.118, avail)    (0.113, techniqu)        (0.042, long)   \n",
       "Topic 13    (0.050, surfac)    (0.050, approxim)        (0.050, mean)   \n",
       "Topic 14     (0.062, known)        (0.062, show)      (0.058, import)   \n",
       "Topic 15      (0.069, valu)       (0.053, exist)      (0.045, calcul)   \n",
       "Topic 16  (0.070, approxim)     (0.066, dataset)        (0.044, solv)   \n",
       "Topic 17     (0.064, avail)    (0.044, challeng)        (0.033, solv)   \n",
       "Topic 18     (0.072, final)  (0.054, experiment)        (0.051, rang)   \n",
       "Topic 19     (0.064, exist)        (0.057, solv)      (0.049, import)   \n",
       "\n",
       "                           6                    7                   8  \\\n",
       "Topic 0       (0.044, avail)       (0.043, appli)       (0.039, rang)   \n",
       "Topic 1        (0.049, type)       (0.045, posit)  (0.034, establish)   \n",
       "Topic 2       (0.041, avail)        (0.041, oper)   (0.041, investig)   \n",
       "Topic 3     (0.045, special)   (0.045, implement)       (0.040, type)   \n",
       "Topic 4       (0.052, finit)     (0.032, multipl)     (0.027, exampl)   \n",
       "Topic 5      (0.049, neural)      (0.040, requir)     (0.038, object)   \n",
       "Topic 6      (0.062, featur)     (0.049, predict)     (0.039, detect)   \n",
       "Topic 7    (0.047, standard)    (0.035, identifi)      (0.021, simpl)   \n",
       "Topic 8    (0.036, standard)      (0.036, strong)       (0.036, lead)   \n",
       "Topic 9        (0.064, real)        (0.052, task)     (0.049, improv)   \n",
       "Topic 10  (0.035, framework)      (0.032, strong)       (0.022, valu)   \n",
       "Topic 11      (0.032, point)     (0.032, multipl)    (0.032, control)   \n",
       "Topic 12       (0.041, solv)       (0.021, reduc)       (0.019, task)   \n",
       "Topic 13      (0.044, group)    (0.040, electron)     (0.037, strong)   \n",
       "Topic 14       (0.049, oper)  (0.046, correspond)       (0.046, mean)   \n",
       "Topic 15   (0.041, investig)      (0.030, specif)       (0.030, lead)   \n",
       "Topic 16       (0.037, real)         (0.034, set)     (0.033, improv)   \n",
       "Topic 17       (0.032, need)        (0.025, show)       (0.025, oper)   \n",
       "Topic 18       (0.042, long)      (0.037, calcul)   (0.037, investig)   \n",
       "Topic 19  (0.046, framework)       (0.046, novel)     (0.036, calcul)   \n",
       "\n",
       "                           9  \n",
       "Topic 0   (0.034, framework)  \n",
       "Topic 1    (0.031, techniqu)  \n",
       "Topic 2        (0.023, real)  \n",
       "Topic 3       (0.027, appli)  \n",
       "Topic 4    (0.020, identifi)  \n",
       "Topic 5        (0.037, need)  \n",
       "Topic 6    (0.034, signific)  \n",
       "Topic 7       (0.020, posit)  \n",
       "Topic 8        (0.036, need)  \n",
       "Topic 9        (0.033, rang)  \n",
       "Topic 10       (0.021, solv)  \n",
       "Topic 11       (0.022, mean)  \n",
       "Topic 12   (0.019, previous)  \n",
       "Topic 13     (0.037, featur)  \n",
       "Topic 14       (0.042, long)  \n",
       "Topic 15   (0.028, signific)  \n",
       "Topic 16      (0.032, bound)  \n",
       "Topic 17    (0.024, possibl)  \n",
       "Topic 18       (0.037, real)  \n",
       "Topic 19       (0.035, valu)  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_topic_model = []\n",
    "for i in range(len(topics)):\n",
    "  str = topics[i].split(' + ')\n",
    "  topic_model = []\n",
    "  for j in range(10):\n",
    "    weight = str[j][0:5]\n",
    "    word = str[j][7:len(str[j])-1]\n",
    "    topic_model.append((weight, word))\n",
    "  all_topic_model.append(topic_model)\n",
    "\n",
    "df_topic_model = pd.DataFrame(all_topic_model)\n",
    "mapper = {idx: f\"Topic {idx}\" for idx in df_topic_model.index} \n",
    "df_topic_model.rename(index = mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/2.6 MB 1.9 MB/s eta 0:00:02\n",
      "     - -------------------------------------- 0.1/2.6 MB 1.6 MB/s eta 0:00:02\n",
      "     - -------------------------------------- 0.1/2.6 MB 1.6 MB/s eta 0:00:02\n",
      "     - -------------------------------------- 0.1/2.6 MB 901.1 kB/s eta 0:00:03\n",
      "     -- ------------------------------------- 0.2/2.6 MB 701.4 kB/s eta 0:00:04\n",
      "     --- ------------------------------------ 0.2/2.6 MB 885.4 kB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 0.3/2.6 MB 1.1 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 0.5/2.6 MB 1.3 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.5/2.6 MB 1.4 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.7/2.6 MB 1.6 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 0.8/2.6 MB 1.7 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 0.9/2.6 MB 1.7 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 1.0/2.6 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 1.2/2.6 MB 1.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.4/2.6 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.6/2.6 MB 2.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.9/2.6 MB 2.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 2.2/2.6 MB 2.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 2.3/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 2.4/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.6/2.6 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\yedidia agnimo\\documents\\youtrend\\youtrend\\venv_youtrend\\lib\\site-packages (from pyLDAvis) (1.26.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\yedidia agnimo\\documents\\youtrend\\youtrend\\venv_youtrend\\lib\\site-packages (from pyLDAvis) (1.11.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\yedidia agnimo\\documents\\youtrend\\youtrend\\venv_youtrend\\lib\\site-packages (from pyLDAvis) (2.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yedidia agnimo\\documents\\youtrend\\youtrend\\venv_youtrend\\lib\\site-packages (from pyLDAvis) (1.3.2)\n",
      "Collecting jinja2 (from pyLDAvis)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting numexpr (from pyLDAvis)\n",
      "  Downloading numexpr-2.8.7-cp310-cp310-win_amd64.whl.metadata (8.9 kB)\n",
      "Collecting funcy (from pyLDAvis)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting scikit-learn>=1.0.0 (from pyLDAvis)\n",
      "  Using cached scikit_learn-1.3.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: gensim in c:\\users\\yedidia agnimo\\documents\\youtrend\\youtrend\\venv_youtrend\\lib\\site-packages (from pyLDAvis) (4.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yedidia agnimo\\documents\\youtrend\\youtrend\\venv_youtrend\\lib\\site-packages (from pyLDAvis) (65.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yedidia agnimo\\documents\\youtrend\\youtrend\\venv_youtrend\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yedidia agnimo\\documents\\youtrend\\youtrend\\venv_youtrend\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\yedidia agnimo\\documents\\youtrend\\youtrend\\venv_youtrend\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=1.0.0->pyLDAvis)\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\yedidia agnimo\\documents\\youtrend\\youtrend\\venv_youtrend\\lib\\site-packages (from gensim->pyLDAvis) (6.4.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->pyLDAvis)\n",
      "  Using cached MarkupSafe-2.1.3-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yedidia agnimo\\documents\\youtrend\\youtrend\\venv_youtrend\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Using cached scikit_learn-1.3.2-cp310-cp310-win_amd64.whl (9.3 MB)\n",
      "Downloading numexpr-2.8.7-cp310-cp310-win_amd64.whl (95 kB)\n",
      "   ---------------------------------------- 0.0/95.3 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/95.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 95.3/95.3 kB 1.8 MB/s eta 0:00:00\n",
      "Using cached MarkupSafe-2.1.3-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: funcy, threadpoolctl, numexpr, MarkupSafe, scikit-learn, jinja2, pyLDAvis\n",
      "Successfully installed MarkupSafe-2.1.3 funcy-2.0 jinja2-3.1.2 numexpr-2.8.7 pyLDAvis-3.4.1 scikit-learn-1.3.2 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el881219972227725441038353824\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el881219972227725441038353824_data = {\"mdsDat\": {\"x\": [-0.053996506523079725, -0.27235284960578227, 0.1252499662858968, 0.004622019957015058, 0.032395393321514365, 0.09296487027865466, 0.11806985527589914, -0.06660261484676384, -0.021940190401800713, 0.0024629141083578974, -0.14264743689195192, -0.15573213980450512, 0.14911670093725785, -0.06402129250137455, -0.08527811134231696, 0.23363610304043916, -0.16826436395365893, 0.16252194813706558, 0.11429387076042426, -0.004498136231291561], \"y\": [-0.008641067299599244, -0.16616285769346614, -0.14628363911923026, 0.08493517575725025, 0.039444213884739995, 0.09828745636738305, 0.19637800719435697, 0.12366227134434005, -0.00886704328957261, -0.0419373612395188, 0.06666353854692683, 0.19472212376811546, -0.10703391181980638, -0.10841895366462222, 0.10803597343185516, -0.019386718482720743, -0.12530127181195755, -0.18240799278544523, 0.15084597516902676, -0.148533918258055], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [9.038942235410776, 7.826608521568066, 7.207336315658595, 6.2196328397275025, 5.894165389926123, 5.869207004249746, 5.264932881737961, 5.171104625482993, 5.154705191309252, 4.947906527567249, 4.698770166810815, 4.385037377562804, 4.208922371038786, 3.716483933193812, 3.6306084832056387, 3.5720033255332564, 3.41226115533366, 3.388646370836945, 3.3462715187146115, 3.0464537651314094]}, \"tinfo\": {\"Term\": [\"system\", \"power\", \"class\", \"detect\", \"equat\", \"approxim\", \"sampl\", \"known\", \"specif\", \"electron\", \"bound\", \"predict\", \"point\", \"object\", \"identifi\", \"neural\", \"group\", \"task\", \"type\", \"control\", \"calcul\", \"featur\", \"implement\", \"standard\", \"dataset\", \"interact\", \"valu\", \"chang\", \"multipl\", \"investig\", \"imag\", \"sampl\", \"detect\", \"dataset\", \"approxim\", \"solv\", \"set\", \"improv\", \"real\", \"techniqu\", \"mean\", \"exist\", \"bound\", \"variabl\", \"random\", \"rang\", \"special\", \"experiment\", \"simpl\", \"finit\", \"exampl\", \"implement\", \"import\", \"lead\", \"long\", \"show\", \"appli\", \"final\", \"possibl\", \"need\", \"predict\", \"reduc\", \"specif\", \"type\", \"multipl\", \"object\", \"dataset\", \"novel\", \"neural\", \"task\", \"import\", \"featur\", \"predict\", \"signific\", \"challeng\", \"specif\", \"detect\", \"experiment\", \"previous\", \"random\", \"show\", \"reduc\", \"appli\", \"final\", \"possibl\", \"need\", \"correspond\", \"rang\", \"special\", \"strong\", \"establish\", \"implement\", \"exampl\", \"variabl\", \"avail\", \"long\", \"exist\", \"finit\", \"imag\", \"improv\", \"sampl\", \"type\", \"multipl\", \"posit\", \"valu\", \"approxim\", \"system\", \"framework\", \"avail\", \"real\", \"challeng\", \"need\", \"show\", \"possibl\", \"solv\", \"requir\", \"rang\", \"oper\", \"establish\", \"special\", \"set\", \"chang\", \"exist\", \"improv\", \"interact\", \"signific\", \"reduc\", \"mean\", \"featur\", \"object\", \"appli\", \"final\", \"correspond\", \"previous\", \"novel\", \"strong\", \"experiment\", \"import\", \"dataset\", \"detect\", \"exampl\", \"finit\", \"equat\", \"surfac\", \"final\", \"lead\", \"rang\", \"experiment\", \"long\", \"investig\", \"calcul\", \"real\", \"mean\", \"solv\", \"type\", \"previous\", \"posit\", \"import\", \"identifi\", \"interact\", \"simpl\", \"reduc\", \"predict\", \"known\", \"approxim\", \"show\", \"appli\", \"possibl\", \"need\", \"correspond\", \"special\", \"novel\", \"dataset\", \"detect\", \"exampl\", \"exist\", \"finit\", \"imag\", \"improv\", \"appli\", \"special\", \"show\", \"valu\", \"correspond\", \"known\", \"import\", \"oper\", \"strong\", \"mean\", \"long\", \"interact\", \"multipl\", \"solv\", \"exampl\", \"signific\", \"implement\", \"random\", \"techniqu\", \"class\", \"previous\", \"reduc\", \"exist\", \"imag\", \"framework\", \"task\", \"featur\", \"equat\", \"final\", \"possibl\", \"dataset\", \"detect\", \"class\", \"posit\", \"correspond\", \"set\", \"random\", \"finit\", \"group\", \"exampl\", \"multipl\", \"identifi\", \"final\", \"novel\", \"standard\", \"framework\", \"need\", \"improv\", \"previous\", \"variabl\", \"requir\", \"show\", \"appli\", \"possibl\", \"rang\", \"special\", \"strong\", \"establish\", \"implement\", \"experiment\", \"reduc\", \"avail\", \"dataset\", \"detect\", \"exist\", \"imag\", \"predict\", \"sampl\", \"specif\", \"type\", \"neural\", \"interact\", \"finit\", \"point\", \"bound\", \"special\", \"implement\", \"lead\", \"equat\", \"appli\", \"type\", \"possibl\", \"exampl\", \"set\", \"oper\", \"group\", \"framework\", \"show\", \"final\", \"previous\", \"exist\", \"techniqu\", \"imag\", \"real\", \"need\", \"correspond\", \"rang\", \"novel\", \"strong\", \"establish\", \"experiment\", \"dataset\", \"detect\", \"improv\", \"predict\", \"reduc\", \"specif\", \"multipl\", \"neural\", \"valu\", \"exampl\", \"set\", \"posit\", \"type\", \"establish\", \"techniqu\", \"experiment\", \"featur\", \"mean\", \"need\", \"sampl\", \"challeng\", \"surfac\", \"real\", \"show\", \"appli\", \"final\", \"possibl\", \"correspond\", \"rang\", \"previous\", \"special\", \"novel\", \"strong\", \"implement\", \"variabl\", \"dataset\", \"detect\", \"exist\", \"finit\", \"imag\", \"improv\", \"predict\", \"reduc\", \"task\", \"approxim\", \"equat\", \"variabl\", \"control\", \"chang\", \"exist\", \"novel\", \"solv\", \"appli\", \"import\", \"framework\", \"calcul\", \"techniqu\", \"valu\", \"point\", \"oper\", \"object\", \"reduc\", \"simpl\", \"signific\", \"lead\", \"predict\", \"show\", \"final\", \"possibl\", \"need\", \"correspond\", \"rang\", \"previous\", \"special\", \"strong\", \"establish\", \"dataset\", \"detect\", \"exampl\", \"experiment\", \"finit\", \"imag\", \"improv\", \"sampl\", \"specif\", \"type\", \"reduc\", \"group\", \"possibl\", \"improv\", \"exampl\", \"surfac\", \"need\", \"requir\", \"neural\", \"simpl\", \"chang\", \"multipl\", \"object\", \"long\", \"show\", \"final\", \"detect\", \"variabl\", \"imag\", \"bound\", \"dataset\", \"power\", \"appli\", \"correspond\", \"rang\", \"previous\", \"special\", \"novel\", \"strong\", \"establish\", \"exist\", \"experiment\", \"finit\", \"predict\", \"sampl\", \"specif\", \"type\", \"posit\", \"calcul\", \"establish\", \"random\", \"mean\", \"surfac\", \"strong\", \"possibl\", \"group\", \"techniqu\", \"approxim\", \"electron\", \"previous\", \"import\", \"simpl\", \"featur\", \"signific\", \"rang\", \"oper\", \"correspond\", \"challeng\", \"task\", \"point\", \"show\", \"appli\", \"final\", \"need\", \"special\", \"novel\", \"implement\", \"exampl\", \"real\", \"dataset\", \"detect\", \"exist\", \"experiment\", \"finit\", \"imag\", \"electron\", \"point\", \"featur\", \"valu\", \"exist\", \"calcul\", \"investig\", \"final\", \"signific\", \"challeng\", \"lead\", \"specif\", \"surfac\", \"mean\", \"correspond\", \"posit\", \"establish\", \"group\", \"import\", \"long\", \"show\", \"appli\", \"possibl\", \"need\", \"rang\", \"previous\", \"special\", \"novel\", \"strong\", \"implement\", \"dataset\", \"detect\", \"exampl\", \"experiment\", \"finit\", \"imag\", \"improv\", \"predict\", \"reduc\", \"identifi\", \"implement\", \"standard\", \"need\", \"appli\", \"signific\", \"rang\", \"oper\", \"avail\", \"establish\", \"experiment\", \"framework\", \"correspond\", \"variabl\", \"featur\", \"sampl\", \"show\", \"final\", \"possibl\", \"previous\", \"special\", \"novel\", \"strong\", \"exampl\", \"reduc\", \"requir\", \"finit\", \"set\", \"posit\", \"simpl\", \"dataset\", \"detect\", \"exist\", \"imag\", \"improv\", \"predict\", \"specif\", \"type\", \"challeng\", \"investig\", \"long\", \"predict\", \"oper\", \"object\", \"real\", \"improv\", \"rang\", \"final\", \"special\", \"task\", \"requir\", \"reduc\", \"variabl\", \"simpl\", \"lead\", \"point\", \"sampl\", \"show\", \"appli\", \"possibl\", \"need\", \"correspond\", \"previous\", \"novel\", \"strong\", \"establish\", \"implement\", \"exampl\", \"multipl\", \"dataset\", \"detect\", \"exist\", \"experiment\", \"finit\", \"imag\", \"specif\", \"type\", \"neural\", \"predict\", \"strong\", \"electron\", \"finit\", \"experiment\", \"sampl\", \"control\", \"multipl\", \"point\", \"previous\", \"mean\", \"exist\", \"requir\", \"chang\", \"interact\", \"known\", \"detect\", \"show\", \"appli\", \"final\", \"possibl\", \"need\", \"correspond\", \"rang\", \"special\", \"novel\", \"establish\", \"implement\", \"exampl\", \"variabl\", \"dataset\", \"imag\", \"improv\", \"reduc\", \"specif\", \"type\", \"neural\", \"posit\", \"task\", \"valu\", \"approxim\", \"equat\", \"power\", \"previous\", \"chang\", \"type\", \"requir\", \"standard\", \"long\", \"identifi\", \"simpl\", \"posit\", \"exist\", \"framework\", \"show\", \"appli\", \"final\", \"possibl\", \"need\", \"correspond\", \"rang\", \"special\", \"novel\", \"strong\", \"establish\", \"implement\", \"exampl\", \"experiment\", \"variabl\", \"reduc\", \"avail\", \"finit\", \"improv\", \"import\", \"dataset\", \"detect\", \"imag\", \"predict\", \"sampl\", \"specif\", \"multipl\", \"neural\", \"task\", \"valu\", \"approxim\", \"equat\", \"approxim\", \"featur\", \"control\", \"improv\", \"task\", \"need\", \"strong\", \"signific\", \"standard\", \"lead\", \"novel\", \"possibl\", \"final\", \"previous\", \"mean\", \"object\", \"show\", \"appli\", \"correspond\", \"rang\", \"special\", \"establish\", \"implement\", \"exampl\", \"experiment\", \"variabl\", \"reduc\", \"avail\", \"requir\", \"finit\", \"dataset\", \"detect\", \"exist\", \"imag\", \"predict\", \"sampl\", \"specif\", \"type\", \"multipl\", \"neural\", \"known\", \"requir\", \"avail\", \"standard\", \"techniqu\", \"long\", \"solv\", \"rang\", \"previous\", \"reduc\", \"experiment\", \"task\", \"show\", \"appli\", \"final\", \"possibl\", \"need\", \"correspond\", \"special\", \"novel\", \"strong\", \"establish\", \"implement\", \"exampl\", \"variabl\", \"finit\", \"set\", \"posit\", \"simpl\", \"identifi\", \"signific\", \"import\", \"lead\", \"dataset\", \"detect\", \"exist\", \"imag\", \"improv\", \"predict\", \"sampl\", \"specif\", \"type\", \"multipl\", \"neural\", \"valu\", \"bound\", \"simpl\", \"type\", \"investig\", \"show\", \"strong\", \"exist\", \"framework\", \"possibl\", \"establish\", \"posit\", \"solv\", \"valu\", \"chang\", \"lead\", \"predict\", \"avail\", \"appli\", \"final\", \"need\", \"correspond\", \"rang\", \"previous\", \"special\", \"novel\", \"implement\", \"exampl\", \"experiment\", \"variabl\", \"reduc\", \"dataset\", \"detect\", \"finit\", \"imag\", \"improv\", \"sampl\", \"specif\", \"multipl\", \"neural\", \"task\", \"detect\", \"system\", \"task\", \"avail\", \"investig\", \"signific\", \"control\", \"interact\", \"oper\", \"previous\", \"novel\", \"exampl\", \"reduc\", \"long\", \"type\", \"real\", \"random\", \"valu\", \"equat\", \"show\", \"appli\", \"final\", \"possibl\", \"need\", \"correspond\", \"rang\", \"special\", \"strong\", \"establish\", \"implement\", \"dataset\", \"exist\", \"experiment\", \"finit\", \"imag\", \"improv\", \"predict\", \"sampl\", \"specif\"], \"Freq\": [48.0, 29.0, 33.0, 38.0, 34.0, 29.0, 38.0, 24.0, 27.0, 26.0, 26.0, 28.0, 31.0, 35.0, 21.0, 28.0, 26.0, 30.0, 23.0, 21.0, 22.0, 29.0, 19.0, 21.0, 25.0, 23.0, 27.0, 23.0, 23.0, 21.0, 19.03998085440244, 27.26861982034198, 16.716507782457107, 9.80881770412821, 10.383351042710071, 6.494266560541942, 5.021073936919449, 4.838355277908357, 5.546999814734925, 4.7359764349301585, 4.6821962170063225, 4.168310101286152, 4.758495903716712, 3.4914868290783225, 3.8471062904629796, 1.9472862818991992, 2.059620487283283, 2.250701043260117, 1.9472969123652266, 1.8206302156738767, 1.1249678938820638, 0.9973760705303083, 1.1676396199520158, 1.0616621571364604, 1.0176040526269037, 0.04749561928911982, 0.04749561928911982, 0.04749561928911982, 0.04749561928911982, 0.04749561928911982, 0.04749561928911982, 0.04749561928911982, 0.04749561928911982, 0.04749561928911982, 0.04749561928911982, 21.202804954066618, 14.177642657025443, 10.014516433292554, 15.668007635301882, 14.8161351111754, 9.318265500869071, 8.01501500203696, 6.330205756636091, 4.330036807368792, 3.7568234069136293, 4.262850354427084, 5.069539655430344, 2.5510494252986247, 1.9132105028409998, 2.4436354488564085, 1.0130809929417692, 1.0131009563207007, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416497999291, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416497999291, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 0.04824416124433069, 39.82749018857506, 13.827923461945787, 7.589559903298858, 8.77099123531403, 5.226967161851766, 3.835871591892367, 2.957008724629791, 2.8887744532034514, 3.878712224795452, 2.8887689490724715, 2.1805968892774588, 2.953709108190009, 2.1381710476850126, 1.9666306225594044, 1.927065608085029, 2.1153828446023653, 1.944805202067747, 1.9416279975009563, 1.9416312999795442, 1.240270492189438, 1.0319464000561767, 1.0307305375227347, 1.010811087506769, 1.0011332840875726, 0.04735733998553406, 0.04735733998553406, 0.04735733998553406, 0.04735733998553406, 0.04735733998553406, 0.04735733998553406, 0.047357346865697786, 0.047357346865697786, 0.04735733998553406, 0.04735733998553406, 0.04735733998553406, 0.04735733998553406, 26.068905906321632, 11.785127226352754, 7.36347190301573, 10.923512086868776, 5.221181731536109, 5.50545340526826, 4.295418711365268, 3.771319020148098, 3.8201507659493714, 3.771304960630129, 2.796855919661826, 1.9089320021755507, 1.5362403049281095, 0.9777092341924234, 1.1313910702033694, 1.3348674440062198, 1.0735706375796508, 1.179637351077336, 1.0115843127837434, 0.9777319384140081, 1.2415643979055913, 1.0167852894680105, 1.1940590916272755, 0.04655971463547018, 0.04655971463547018, 0.04655971463547018, 0.04655971463547018, 0.04655971463547018, 0.04655971463547018, 0.04655971463547018, 0.04655971463547018, 0.04655971463547018, 0.04655971463547018, 0.04655971463547018, 0.04655971463547018, 0.04655971463547018, 0.04655971463547018, 6.708030432706133, 7.521906998897907, 5.969310163415217, 10.286631568145921, 4.477054643084728, 6.029736822887548, 5.607124713302467, 4.739810719323533, 3.7169640840129623, 4.409097157591891, 4.0707137702297445, 3.742985819292155, 3.6934828726937305, 3.686332675138949, 2.1752981634565103, 2.2121141547189507, 1.9505568600907357, 1.9505537992184947, 1.4471939893792887, 1.9505534391158783, 0.9990533050484192, 1.1440028005557763, 0.9990533050484192, 0.9990522247405694, 1.0667053334967436, 1.105020972105925, 1.0517236242366135, 1.0512317240623672, 0.04757495156265979, 0.04757495156265979, 0.04757495156265979, 0.04757495156265979, 30.412858449714005, 12.115568996398958, 8.845845146195051, 6.916755442700806, 6.231920630361856, 5.034054230744527, 5.3093505361354705, 2.6251263067042014, 3.053818784172259, 1.9452637132904191, 1.3328200195097724, 1.3122194565403766, 1.4747525492847262, 1.945258872490254, 0.9963455023430214, 1.3197890336874798, 0.9963455023430214, 1.0258825409950372, 1.0361564221899455, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 0.04744600467403977, 13.210673813262783, 9.308234622292021, 10.552754431854956, 7.687744032950143, 3.8677401391977715, 3.8677311327231516, 4.9052947002485965, 5.777734527068473, 2.3161288161317968, 3.4567914285924224, 1.95773754900197, 2.2285279840589682, 1.95773754900197, 1.9577298291665812, 1.8478027506600339, 1.9577370665122582, 1.0027220204575713, 1.0027331177209424, 1.0027327156461827, 1.1740457539596576, 1.1503645960002769, 1.0027327156461827, 1.002711244854008, 0.047750081843072334, 0.04775008686900683, 0.047750081843072334, 0.047750081843072334, 0.047750081843072334, 0.047750081843072334, 0.047750081843072334, 0.047750081843072334, 0.047750081843072334, 0.047750081843072334, 0.047750081843072334, 0.047750081843072334, 20.5766417668853, 12.348852417151106, 8.466119288408274, 6.7336966762402, 4.312066766674735, 4.38546491420794, 3.807786621055962, 4.177094054295712, 2.8530970051830846, 2.635271070249664, 2.32448192025472, 2.1678914021106053, 1.4490842353973352, 1.003109201014864, 2.050362936669909, 1.1092438240046587, 1.0143350490744756, 1.0031050939587534, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 0.04776798489368386, 12.019056378709498, 11.686181412783586, 12.306743318275066, 5.404754127707332, 3.896229616468212, 4.860215861401287, 2.5939447591023557, 4.134540020874146, 3.9277051578557205, 3.005951678638436, 2.4732835832844855, 2.9815868395200384, 2.957902869275631, 1.9880901881549111, 2.743146685818968, 1.1048553223487843, 1.0182824671104012, 1.0182726256893502, 1.0457537311006937, 1.0431152854826145, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 0.04849049726028337, 11.238152665677775, 13.632869370805949, 6.4603616971198345, 5.9600299265125996, 5.078237546923285, 4.625918076520509, 2.9694758842883076, 3.2196320962748364, 3.9430973652905332, 2.7620781521625104, 2.145936371351914, 2.116687585678841, 3.1118096765117778, 1.995881609490394, 1.1066890667555371, 1.0222579368772657, 2.1016484493721324, 1.0222699529508366, 1.0281009226899183, 1.134303137413432, 1.1004135022943302, 1.130510365814027, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 0.04868035421928886, 11.974027826371419, 9.444788074784435, 8.56368742765061, 3.8323571809756367, 3.883479509277246, 2.8860973323576746, 2.302341592458207, 3.380276107360203, 2.8860956099362767, 3.8323643577314614, 3.0634045393427276, 1.9398247091143448, 2.6717643808620006, 2.0701913398817577, 2.886096758217209, 1.9398247091143448, 1.4127456759194992, 1.9398280104220242, 0.9935456985583307, 0.9935545259679952, 0.9935545259679952, 0.9935619180264947, 0.04731326963309183, 0.04731326963309183, 0.04731326963309183, 0.04731326963309183, 0.04731326963309183, 0.04731326963309183, 0.04731326963309183, 0.04731326963309183, 0.047313274118564216, 0.04731326963309183, 0.04731326963309183, 0.04731326963309183, 0.04731326963309183, 0.04731326963309183, 0.04731326963309183, 15.775475885096153, 13.736852354533163, 5.345504291915522, 4.937694071227982, 3.815347423447639, 3.269605033147999, 2.9821648816785644, 2.0044004208552577, 2.0044022961748835, 2.00438756152068, 2.138213316957609, 2.1450097431847692, 1.639628919534124, 1.5559721165841762, 1.0976811715218446, 1.0894777867453336, 0.8858928872276904, 1.119750133806846, 1.026635960031951, 0.9223624958958796, 0.12040190779193317, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 0.048888176158174694, 15.396357725838365, 11.63386457607885, 7.771995903991136, 4.875588999920255, 2.9446526067315992, 3.9100542032630927, 2.701046805470846, 3.796714267760037, 3.049852677412166, 1.9791919958587143, 1.9791675672642384, 2.345391155638467, 1.0137263064096091, 1.0137214849764888, 1.0137263064096091, 1.091053543650432, 0.04827340581185545, 0.04827340581185545, 0.048273401793994516, 0.04827340581185545, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 0.048273401793994516, 8.473089180972671, 6.923887264715362, 4.910131668549672, 4.960108428871863, 4.093279815121692, 5.699069957572462, 3.9131240683315185, 2.995699053818099, 2.013494479820063, 1.8050773162407854, 2.013492436300836, 3.1810460206170244, 1.2670814853171986, 1.1181365212044057, 1.0312987610720088, 1.0520259495301678, 1.0312987610720088, 1.0312987610720088, 1.0312987610720088, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.04911006337351993, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 0.0491100598257435, 13.161097266849465, 7.850524834596984, 7.257358339455258, 3.9480118126587143, 3.630640867554951, 6.733806387089847, 1.9126764536984668, 1.912686435200302, 1.9133598538574423, 1.0423005842366269, 1.3315560785936258, 1.0766667841492494, 0.9796550151359925, 1.06706724111772, 1.0880839586649478, 1.0428467941981578, 1.2144303634213367, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 0.04665120709429439, 27.57268222805201, 4.9838083262236355, 3.9267036945789644, 3.996917671272215, 3.104743715969524, 2.728709551651383, 3.0100241793219045, 2.023127195680652, 1.2464686335846553, 1.151902731338488, 1.0362285753092697, 1.0362285753092697, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934480144410054, 0.04934480144410054, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 0.04934479803424611, 13.625247208120477, 7.903623829452329, 4.937107197365752, 4.491293878672231, 5.32280990879279, 2.004147977851413, 2.0041523557481753, 2.031545064264516, 2.0041592353002313, 2.0041521472769013, 1.3887884114475075, 1.057195878595302, 1.0265162034951694, 1.0265162034951694, 1.1828227532740017, 1.4091253054454396, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 0.04888233791145461, 15.665297707537066, 7.42076889615536, 6.568620814174303, 6.836115759131556, 6.2848252704976675, 2.346058329350838, 2.2583003826307237, 1.050790408448703, 1.0507995177042362, 1.1884625208623445, 1.0507995177042362, 1.0507995177042362, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038664243847106, 0.050038664243847106, 0.050038664243847106, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 0.050038657774205395, 12.285150037993063, 9.330482675527342, 9.049704437725712, 4.781920363416409, 2.4564105145607398, 1.7321078084350936, 1.9409269681374224, 1.9409253326201947, 0.9941183786103577, 1.059323077219277, 0.9941280894938963, 1.1605230567311626, 1.1988263591017099, 0.9941280894938963, 0.9941104054638732, 1.112067690049784, 0.4396613255225829, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 0.0473399854589809, 13.083685703264097, 8.14083791341821, 3.0844124897583325, 2.073123161436551, 2.073121300212952, 2.07312520878251, 2.07312520878251, 2.07312520878251, 2.073123161436551, 1.0618280633216122, 1.0618388584184872, 1.0618388584184872, 1.0618388584184872, 1.1132818689039705, 1.061837369439608, 1.1285363644615178, 1.0618388584184872, 1.0618388584184872, 1.0618388584184872, 0.050564262844757915, 0.05056425702843417, 0.050564262844757915, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417, 0.05056425702843417], \"Total\": [48.0, 29.0, 33.0, 38.0, 34.0, 29.0, 38.0, 24.0, 27.0, 26.0, 26.0, 28.0, 31.0, 35.0, 21.0, 28.0, 26.0, 30.0, 23.0, 21.0, 22.0, 29.0, 19.0, 21.0, 25.0, 23.0, 27.0, 23.0, 23.0, 21.0, 22.842138570885076, 38.89961603624745, 38.90794921538965, 25.90622658901535, 29.80854361904025, 24.876198857768923, 20.934053280549847, 22.221596315924494, 25.763940589589403, 22.240352352341798, 22.805954455764166, 22.093018444746626, 26.63819993158299, 20.277992871039828, 24.773877051857262, 17.15476706975482, 18.15387549205317, 19.92103586708031, 21.017849004909678, 20.885360828788713, 19.232557126928057, 19.222207445254302, 25.890044127194503, 24.68325370053237, 24.206992961263, 15.253561210234805, 15.334440542531976, 16.183733058601966, 16.336979040257802, 16.359904597887994, 28.525536060596405, 20.40538085064261, 27.803374402521065, 23.953031304679996, 23.851180456320147, 35.84009797337631, 25.90622658901535, 18.393738378368695, 28.89630464896349, 30.175823711461888, 25.890044127194503, 29.973863539022794, 28.525536060596405, 21.287830035777667, 22.239157529810946, 27.803374402521065, 38.90794921538965, 19.92103586708031, 17.42753208706933, 24.773877051857262, 15.253561210234805, 20.40538085064261, 15.334440542531976, 16.183733058601966, 16.336979040257802, 16.359904597887994, 17.15213002729364, 17.15476706975482, 18.15387549205317, 18.91585752381051, 19.037296790354855, 19.222207445254302, 19.232557126928057, 20.277992871039828, 20.441017100123414, 24.206992961263, 22.093018444746626, 20.885360828788713, 22.842138570885076, 22.221596315924494, 38.89961603624745, 23.953031304679996, 23.851180456320147, 20.966681491701515, 27.87342138061222, 29.80854361904025, 48.8341793652994, 28.628070755575994, 20.441017100123414, 25.763940589589403, 22.239157529810946, 16.359904597887994, 15.253561210234805, 16.336979040257802, 24.876198857768923, 20.54195101882808, 17.15476706975482, 24.119624100962625, 19.037296790354855, 18.15387549205317, 20.934053280549847, 23.231870237687403, 22.093018444746626, 22.221596315924494, 23.913452759229916, 21.287830035777667, 20.40538085064261, 22.805954455764166, 29.973863539022794, 35.84009797337631, 15.334440542531976, 16.183733058601966, 17.15213002729364, 17.42753208706933, 18.393738378368695, 18.91585752381051, 19.92103586708031, 25.890044127194503, 25.90622658901535, 38.90794921538965, 19.232557126928057, 20.885360828788713, 34.73103487112142, 23.673052141539504, 16.183733058601966, 24.68325370053237, 17.15476706975482, 19.92103586708031, 24.206992961263, 21.253723497384684, 22.84225650674031, 25.763940589589403, 22.805954455764166, 24.876198857768923, 23.953031304679996, 17.42753208706933, 20.966681491701515, 25.890044127194503, 21.21046821357144, 23.913452759229916, 21.017849004909678, 20.40538085064261, 28.525536060596405, 24.527614943344254, 29.80854361904025, 15.253561210234805, 15.334440542531976, 16.336979040257802, 16.359904597887994, 17.15213002729364, 18.15387549205317, 18.393738378368695, 25.90622658901535, 38.90794921538965, 19.232557126928057, 22.093018444746626, 20.885360828788713, 22.842138570885076, 22.221596315924494, 15.334440542531976, 18.15387549205317, 15.253561210234805, 27.87342138061222, 17.15213002729364, 24.527614943344254, 25.890044127194503, 24.119624100962625, 18.91585752381051, 22.805954455764166, 24.206992961263, 23.913452759229916, 23.851180456320147, 24.876198857768923, 19.232557126928057, 21.287830035777667, 19.222207445254302, 24.773877051857262, 22.240352352341798, 33.232163792913276, 17.42753208706933, 20.40538085064261, 22.093018444746626, 22.842138570885076, 28.628070755575994, 30.175823711461888, 29.973863539022794, 34.73103487112142, 16.183733058601966, 16.336979040257802, 25.90622658901535, 38.90794921538965, 33.232163792913276, 20.966681491701515, 17.15213002729364, 20.934053280549847, 24.773877051857262, 20.885360828788713, 26.013743872560937, 19.232557126928057, 23.851180456320147, 21.21046821357144, 16.183733058601966, 18.393738378368695, 21.53552065949119, 28.628070755575994, 16.359904597887994, 22.221596315924494, 17.42753208706933, 20.277992871039828, 20.54195101882808, 15.253561210234805, 15.334440542531976, 16.336979040257802, 17.15476706975482, 18.15387549205317, 18.91585752381051, 19.037296790354855, 19.222207445254302, 19.92103586708031, 20.40538085064261, 20.441017100123414, 25.90622658901535, 38.90794921538965, 22.093018444746626, 22.842138570885076, 28.525536060596405, 38.89961603624745, 27.803374402521065, 23.953031304679996, 28.89630464896349, 23.913452759229916, 20.885360828788713, 31.861299757125135, 26.63819993158299, 18.15387549205317, 19.222207445254302, 24.68325370053237, 34.73103487112142, 15.334440542531976, 23.953031304679996, 16.336979040257802, 19.232557126928057, 20.934053280549847, 24.119624100962625, 26.013743872560937, 28.628070755575994, 15.253561210234805, 16.183733058601966, 17.42753208706933, 22.093018444746626, 22.240352352341798, 22.842138570885076, 25.763940589589403, 16.359904597887994, 17.15213002729364, 17.15476706975482, 18.393738378368695, 18.91585752381051, 19.037296790354855, 19.92103586708031, 25.90622658901535, 38.90794921538965, 22.221596315924494, 28.525536060596405, 20.40538085064261, 27.803374402521065, 23.851180456320147, 28.89630464896349, 27.87342138061222, 19.232557126928057, 20.934053280549847, 20.966681491701515, 23.953031304679996, 19.037296790354855, 22.240352352341798, 19.92103586708031, 29.973863539022794, 22.805954455764166, 16.359904597887994, 38.89961603624745, 22.239157529810946, 23.673052141539504, 25.763940589589403, 15.253561210234805, 15.334440542531976, 16.183733058601966, 16.336979040257802, 17.15213002729364, 17.15476706975482, 17.42753208706933, 18.15387549205317, 18.393738378368695, 18.91585752381051, 19.222207445254302, 20.277992871039828, 25.90622658901535, 38.90794921538965, 22.093018444746626, 20.885360828788713, 22.842138570885076, 22.221596315924494, 28.525536060596405, 20.40538085064261, 30.175823711461888, 29.80854361904025, 34.73103487112142, 20.277992871039828, 21.37827483365595, 23.231870237687403, 22.093018444746626, 18.393738378368695, 24.876198857768923, 15.334440542531976, 25.890044127194503, 28.628070755575994, 22.84225650674031, 22.240352352341798, 27.87342138061222, 31.861299757125135, 24.119624100962625, 35.84009797337631, 20.40538085064261, 21.017849004909678, 21.287830035777667, 24.68325370053237, 28.525536060596405, 15.253561210234805, 16.183733058601966, 16.336979040257802, 16.359904597887994, 17.15213002729364, 17.15476706975482, 17.42753208706933, 18.15387549205317, 18.91585752381051, 19.037296790354855, 25.90622658901535, 38.90794921538965, 19.232557126928057, 19.92103586708031, 20.885360828788713, 22.842138570885076, 22.221596315924494, 38.89961603624745, 27.803374402521065, 23.953031304679996, 20.40538085064261, 26.013743872560937, 16.336979040257802, 22.221596315924494, 19.232557126928057, 23.673052141539504, 16.359904597887994, 20.54195101882808, 28.89630464896349, 21.017849004909678, 23.231870237687403, 23.851180456320147, 35.84009797337631, 24.206992961263, 15.253561210234805, 16.183733058601966, 38.90794921538965, 20.277992871039828, 22.842138570885076, 26.63819993158299, 25.90622658901535, 29.568940301932603, 15.334440542531976, 17.15213002729364, 17.15476706975482, 17.42753208706933, 18.15387549205317, 18.393738378368695, 18.91585752381051, 19.037296790354855, 22.093018444746626, 19.92103586708031, 20.885360828788713, 28.525536060596405, 38.89961603624745, 27.803374402521065, 23.953031304679996, 20.966681491701515, 22.84225650674031, 19.037296790354855, 24.773877051857262, 22.805954455764166, 23.673052141539504, 18.91585752381051, 16.336979040257802, 26.013743872560937, 22.240352352341798, 29.80854361904025, 26.91715897132868, 17.42753208706933, 25.890044127194503, 21.017849004909678, 29.973863539022794, 21.287830035777667, 17.15476706975482, 24.119624100962625, 17.15213002729364, 22.239157529810946, 30.175823711461888, 31.861299757125135, 15.253561210234805, 15.334440542531976, 16.183733058601966, 16.359904597887994, 18.15387549205317, 18.393738378368695, 19.222207445254302, 19.232557126928057, 25.763940589589403, 25.90622658901535, 38.90794921538965, 22.093018444746626, 19.92103586708031, 20.885360828788713, 22.842138570885076, 26.91715897132868, 31.861299757125135, 29.973863539022794, 27.87342138061222, 22.093018444746626, 22.84225650674031, 21.253723497384684, 16.183733058601966, 21.287830035777667, 22.239157529810946, 24.68325370053237, 27.803374402521065, 23.673052141539504, 22.805954455764166, 17.15213002729364, 20.966681491701515, 19.037296790354855, 26.013743872560937, 25.890044127194503, 24.206992961263, 15.253561210234805, 15.334440542531976, 16.336979040257802, 16.359904597887994, 17.15476706975482, 17.42753208706933, 18.15387549205317, 18.393738378368695, 18.91585752381051, 19.222207445254302, 25.90622658901535, 38.90794921538965, 19.232557126928057, 19.92103586708031, 20.885360828788713, 22.842138570885076, 22.221596315924494, 28.525536060596405, 20.40538085064261, 21.21046821357144, 19.222207445254302, 21.53552065949119, 16.359904597887994, 15.334440542531976, 21.287830035777667, 17.15476706975482, 24.119624100962625, 20.441017100123414, 19.037296790354855, 19.92103586708031, 28.628070755575994, 17.15213002729364, 20.277992871039828, 29.973863539022794, 38.89961603624745, 15.253561210234805, 16.183733058601966, 16.336979040257802, 17.42753208706933, 18.15387549205317, 18.393738378368695, 18.91585752381051, 19.232557126928057, 20.40538085064261, 20.54195101882808, 20.885360828788713, 20.934053280549847, 20.966681491701515, 21.017849004909678, 25.90622658901535, 38.90794921538965, 22.093018444746626, 22.842138570885076, 22.221596315924494, 28.525536060596405, 27.803374402521065, 23.953031304679996, 22.239157529810946, 21.253723497384684, 24.206992961263, 28.525536060596405, 24.119624100962625, 35.84009797337631, 25.763940589589403, 22.221596315924494, 17.15476706975482, 16.183733058601966, 18.15387549205317, 30.175823711461888, 20.54195101882808, 20.40538085064261, 20.277992871039828, 21.017849004909678, 24.68325370053237, 31.861299757125135, 38.89961603624745, 15.253561210234805, 15.334440542531976, 16.336979040257802, 16.359904597887994, 17.15213002729364, 17.42753208706933, 18.393738378368695, 18.91585752381051, 19.037296790354855, 19.222207445254302, 19.232557126928057, 23.851180456320147, 25.90622658901535, 38.90794921538965, 22.093018444746626, 19.92103586708031, 20.885360828788713, 22.842138570885076, 27.803374402521065, 23.953031304679996, 28.89630464896349, 28.525536060596405, 18.91585752381051, 26.91715897132868, 20.885360828788713, 19.92103586708031, 38.89961603624745, 21.37827483365595, 23.851180456320147, 31.861299757125135, 17.42753208706933, 22.805954455764166, 22.093018444746626, 20.54195101882808, 23.231870237687403, 23.913452759229916, 24.527614943344254, 38.90794921538965, 15.253561210234805, 15.334440542531976, 16.183733058601966, 16.336979040257802, 16.359904597887994, 17.15213002729364, 17.15476706975482, 18.15387549205317, 18.393738378368695, 19.037296790354855, 19.222207445254302, 19.232557126928057, 20.277992871039828, 25.90622658901535, 22.842138570885076, 22.221596315924494, 20.40538085064261, 27.803374402521065, 23.953031304679996, 28.89630464896349, 20.966681491701515, 30.175823711461888, 27.87342138061222, 29.80854361904025, 34.73103487112142, 29.568940301932603, 17.42753208706933, 23.231870237687403, 23.953031304679996, 20.54195101882808, 21.53552065949119, 24.206992961263, 21.21046821357144, 21.017849004909678, 20.966681491701515, 22.093018444746626, 28.628070755575994, 15.253561210234805, 15.334440542531976, 16.183733058601966, 16.336979040257802, 16.359904597887994, 17.15213002729364, 17.15476706975482, 18.15387549205317, 18.393738378368695, 18.91585752381051, 19.037296790354855, 19.222207445254302, 19.232557126928057, 19.92103586708031, 20.277992871039828, 20.40538085064261, 20.441017100123414, 20.885360828788713, 22.221596315924494, 25.890044127194503, 25.90622658901535, 38.90794921538965, 22.842138570885076, 28.525536060596405, 38.89961603624745, 27.803374402521065, 23.851180456320147, 28.89630464896349, 30.175823711461888, 27.87342138061222, 29.80854361904025, 34.73103487112142, 29.80854361904025, 29.973863539022794, 21.37827483365595, 22.221596315924494, 30.175823711461888, 16.359904597887994, 18.91585752381051, 21.287830035777667, 21.53552065949119, 24.68325370053237, 18.393738378368695, 16.336979040257802, 16.183733058601966, 17.42753208706933, 22.805954455764166, 35.84009797337631, 15.253561210234805, 15.334440542531976, 17.15213002729364, 17.15476706975482, 18.15387549205317, 19.037296790354855, 19.222207445254302, 19.232557126928057, 19.92103586708031, 20.277992871039828, 20.40538085064261, 20.441017100123414, 20.54195101882808, 20.885360828788713, 25.90622658901535, 38.90794921538965, 22.093018444746626, 22.842138570885076, 28.525536060596405, 38.89961603624745, 27.803374402521065, 23.953031304679996, 23.851180456320147, 28.89630464896349, 24.527614943344254, 20.54195101882808, 20.441017100123414, 21.53552065949119, 22.240352352341798, 24.206992961263, 24.876198857768923, 17.15476706975482, 17.42753208706933, 20.40538085064261, 19.92103586708031, 30.175823711461888, 15.253561210234805, 15.334440542531976, 16.183733058601966, 16.336979040257802, 16.359904597887994, 17.15213002729364, 18.15387549205317, 18.393738378368695, 18.91585752381051, 19.037296790354855, 19.222207445254302, 19.232557126928057, 20.277992871039828, 20.885360828788713, 20.934053280549847, 20.966681491701515, 21.017849004909678, 21.21046821357144, 21.287830035777667, 25.890044127194503, 24.68325370053237, 25.90622658901535, 38.90794921538965, 22.093018444746626, 22.842138570885076, 22.221596315924494, 28.525536060596405, 38.89961603624745, 27.803374402521065, 23.953031304679996, 23.851180456320147, 28.89630464896349, 27.87342138061222, 26.63819993158299, 21.017849004909678, 23.953031304679996, 21.253723497384684, 15.253561210234805, 18.91585752381051, 22.093018444746626, 28.628070755575994, 16.336979040257802, 19.037296790354855, 20.966681491701515, 24.876198857768923, 27.87342138061222, 23.231870237687403, 24.68325370053237, 28.525536060596405, 20.441017100123414, 15.334440542531976, 16.183733058601966, 16.359904597887994, 17.15213002729364, 17.15476706975482, 17.42753208706933, 18.15387549205317, 18.393738378368695, 19.222207445254302, 19.232557126928057, 19.92103586708031, 20.277992871039828, 20.40538085064261, 25.90622658901535, 38.90794921538965, 20.885360828788713, 22.842138570885076, 22.221596315924494, 38.89961603624745, 27.803374402521065, 23.851180456320147, 28.89630464896349, 30.175823711461888, 38.90794921538965, 48.8341793652994, 30.175823711461888, 20.441017100123414, 21.253723497384684, 21.287830035777667, 21.37827483365595, 23.913452759229916, 24.119624100962625, 17.42753208706933, 18.393738378368695, 19.232557126928057, 20.40538085064261, 24.206992961263, 23.953031304679996, 25.763940589589403, 24.773877051857262, 27.87342138061222, 34.73103487112142, 15.253561210234805, 15.334440542531976, 16.183733058601966, 16.336979040257802, 16.359904597887994, 17.15213002729364, 17.15476706975482, 18.15387549205317, 18.91585752381051, 19.037296790354855, 19.222207445254302, 25.90622658901535, 22.093018444746626, 19.92103586708031, 20.885360828788713, 22.842138570885076, 22.221596315924494, 28.525536060596405, 38.89961603624745, 27.803374402521065], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.0523, -1.6931, -2.1824, -2.7155, -2.6586, -3.1279, -3.3852, -3.4222, -3.2856, -3.4436, -3.4551, -3.5713, -3.4389, -3.7485, -3.6515, -4.3324, -4.2763, -4.1876, -4.3324, -4.3996, -4.8811, -5.0015, -4.8438, -4.939, -4.9814, -8.0459, -8.0459, -8.0459, -8.0459, -8.0459, -8.0459, -8.0459, -8.0459, -8.0459, -8.0459, -1.8007, -2.2031, -2.5508, -2.1032, -2.1591, -2.6228, -2.7735, -3.0095, -3.3892, -3.5312, -3.4049, -3.2316, -3.9183, -4.206, -3.9613, -4.8418, -4.8418, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -7.8863, -1.0878, -2.1457, -2.7456, -2.6009, -3.1185, -3.428, -3.6882, -3.7115, -3.4169, -3.7116, -3.9928, -3.6893, -4.0124, -4.0961, -4.1164, -4.0231, -4.1072, -4.1089, -4.1089, -4.5571, -4.7409, -4.7421, -4.7616, -4.7712, -7.8224, -7.8224, -7.8224, -7.8224, -7.8224, -7.8224, -7.8224, -7.8224, -7.8224, -7.8224, -7.8224, -7.8224, -1.3642, -2.1582, -2.6285, -2.2341, -2.9723, -2.9193, -3.1674, -3.2976, -3.2847, -3.2976, -3.5965, -3.9784, -4.1957, -4.6475, -4.5015, -4.3362, -4.554, -4.4598, -4.6135, -4.6475, -4.4086, -4.6083, -4.4476, -7.692, -7.692, -7.692, -7.692, -7.692, -7.692, -7.692, -7.692, -7.692, -7.692, -7.692, -7.692, -7.692, -7.692, -2.6679, -2.5534, -2.7846, -2.2404, -3.0723, -2.7745, -2.8472, -3.0152, -3.2583, -3.0876, -3.1674, -3.2514, -3.2647, -3.2666, -3.7941, -3.7773, -3.9031, -3.9031, -4.2016, -3.9031, -4.5722, -4.4367, -4.5722, -4.5722, -4.5067, -4.4714, -4.5208, -4.5213, -7.6167, -7.6167, -7.6167, -7.6167, -1.1521, -2.0725, -2.3871, -2.6331, -2.7373, -2.9508, -2.8975, -3.6019, -3.4506, -3.9016, -4.2797, -4.2953, -4.1785, -3.9016, -4.5707, -4.2895, -4.5707, -4.5414, -4.5315, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -7.6152, -1.8773, -2.2275, -2.102, -2.4187, -3.1057, -3.1057, -2.868, -2.7043, -3.6185, -3.218, -3.7866, -3.657, -3.7866, -3.7866, -3.8444, -3.7866, -4.4556, -4.4556, -4.4556, -4.2979, -4.3183, -4.4556, -4.4556, -7.5001, -7.5001, -7.5001, -7.5001, -7.5001, -7.5001, -7.5001, -7.5001, -7.5001, -7.5001, -7.5001, -7.5001, -1.4162, -1.9268, -2.3043, -2.5332, -2.979, -2.9621, -3.1033, -3.0108, -3.392, -3.4714, -3.5969, -3.6666, -4.0694, -4.4373, -3.7224, -4.3367, -4.4261, -4.4373, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -7.4818, -1.9507, -1.9788, -1.927, -2.7499, -3.0772, -2.8561, -3.484, -3.0178, -3.0691, -3.3366, -3.5316, -3.3447, -3.3527, -3.75, -3.4281, -4.3375, -4.4191, -4.4191, -4.3925, -4.395, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -7.4636, -1.9769, -1.7838, -2.5306, -2.6112, -2.7713, -2.8646, -3.3079, -3.227, -3.0243, -3.3803, -3.6327, -3.6464, -3.261, -3.7052, -4.2949, -4.3742, -3.6535, -4.3742, -4.3685, -4.2702, -4.3006, -4.2736, -7.4187, -7.4187, -7.4187, -7.4187, -7.4187, -7.4187, -7.4187, -7.4187, -7.4187, -7.4187, -7.4187, -7.4187, -7.4187, -7.4187, -7.4187, -7.4187, -1.8618, -2.0991, -2.1971, -3.0011, -2.9879, -3.2847, -3.5107, -3.1266, -3.2847, -3.0011, -3.2251, -3.682, -3.3618, -3.6169, -3.2847, -3.682, -3.999, -3.682, -4.3511, -4.351, -4.351, -4.351, -7.3955, -7.3955, -7.3955, -7.3955, -7.3955, -7.3955, -7.3955, -7.3955, -7.3955, -7.3955, -7.3955, -7.3955, -7.3955, -7.3955, -7.3955, -1.517, -1.6554, -2.5992, -2.6786, -2.9364, -3.0908, -3.1828, -3.5801, -3.5801, -3.5801, -3.5155, -3.5123, -3.781, -3.8334, -4.1823, -4.1898, -4.3966, -4.1624, -4.2492, -4.3563, -6.3924, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -7.2937, -1.5004, -1.7806, -2.184, -2.6502, -3.1545, -2.8709, -3.2408, -2.9004, -3.1194, -3.5518, -3.5518, -3.382, -4.2209, -4.2209, -4.2209, -4.1473, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -7.2654, -1.9732, -2.1751, -2.5188, -2.5086, -2.7007, -2.3698, -2.7457, -3.0129, -3.4102, -3.5195, -3.4102, -2.9528, -3.8733, -3.9984, -4.0792, -4.0593, -4.0792, -4.0792, -4.0792, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -7.1238, -1.5094, -2.0261, -2.1047, -2.7135, -2.7973, -2.1795, -3.4382, -3.4382, -3.4378, -4.0453, -3.8003, -4.0128, -4.1072, -4.0218, -4.0023, -4.0447, -3.8924, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -7.1517, -0.7536, -2.4642, -2.7026, -2.6849, -2.9375, -3.0666, -2.9685, -3.3658, -3.8501, -3.929, -4.0348, -4.0348, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -7.0793, -1.4127, -1.9573, -2.4279, -2.5225, -2.3527, -3.3294, -3.3294, -3.3159, -3.3294, -3.3294, -3.6962, -3.969, -3.9985, -3.9985, -3.8568, -3.6817, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -7.043, -1.2663, -2.0134, -2.1354, -2.0955, -2.1796, -3.165, -3.2031, -3.9682, -3.9682, -3.8451, -3.9682, -3.9682, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -7.0127, -1.4967, -1.7718, -1.8024, -2.4403, -3.1064, -3.4558, -3.342, -3.342, -4.011, -3.9475, -4.011, -3.8563, -3.8238, -4.011, -4.011, -3.8989, -4.8269, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -7.0555, -1.3399, -1.8144, -2.7849, -3.1822, -3.1822, -3.1822, -3.1822, -3.1822, -3.1822, -3.8513, -3.8513, -3.8513, -3.8513, -3.8039, -3.8513, -3.7903, -3.8513, -3.8513, -3.8513, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958, -6.8958], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.2216, 2.0484, 1.5588, 1.4324, 1.349, 1.0606, 0.9759, 0.8791, 0.8679, 0.8569, 0.8204, 0.7359, 0.6812, 0.6444, 0.5412, 0.2278, 0.2273, 0.2231, 0.0247, -0.0362, -0.4352, -0.5551, -0.6952, -0.7427, -0.7656, -3.3683, -3.3736, -3.4275, -3.4369, -3.4383, -3.9943, -3.6593, -3.9686, -3.8196, -3.8153, 2.0227, 1.9448, 1.9397, 1.9355, 1.8363, 1.5258, 1.2286, 1.0422, 0.9551, 0.7694, 0.6724, 0.5097, 0.4924, 0.3384, 0.2313, -0.1642, -0.4551, -3.2139, -3.2678, -3.2773, -3.2787, -3.326, -3.3261, -3.3827, -3.4238, -3.4302, -3.4399, -3.4404, -3.4934, -3.5014, -3.6705, -3.5791, -3.5229, -3.6124, -3.5849, -4.1448, -3.6599, -3.6557, -3.5268, -3.8115, -3.8786, 2.4262, 1.9024, 1.6393, 1.5525, 1.182, 1.1796, 0.9894, 0.8975, 0.7717, 0.6684, 0.5674, 0.5301, 0.4436, 0.4075, 0.2447, 0.2338, 0.2, 0.1925, 0.1192, -0.2127, -0.3543, -0.4667, -0.7595, -0.9479, -3.1501, -3.204, -3.2621, -3.278, -3.332, -3.36, -3.4117, -3.6738, -3.6744, -4.0812, -3.3766, -3.459, 2.4906, 2.08, 1.99, 1.9623, 1.5879, 1.4914, 1.0484, 1.0484, 0.9891, 0.8559, 0.6789, 0.2101, 0.0307, -0.1031, -0.142, -0.1876, -0.206, -0.2318, -0.2564, -0.2609, -0.357, -0.4057, -0.44, -3.0144, -3.0197, -3.083, -3.0844, -3.1317, -3.1884, -3.2016, -3.544, -3.9508, -3.2462, -3.3848, -3.3286, -3.4182, -3.3906, 2.0044, 1.9501, 1.893, 1.8344, 1.4881, 1.4281, 1.3014, 1.2042, 1.2041, 1.1879, 1.0484, 0.9766, 0.9659, 0.9219, 0.6518, 0.567, 0.5433, 0.2895, 0.0989, -0.0042, -0.0278, -0.0501, -0.265, -0.2983, -0.4586, -0.476, -0.5187, -0.6665, -2.9982, -3.0077, -3.4687, -3.8754, 2.7468, 2.287, 2.1733, 1.728, 1.4553, 1.4126, 1.2463, 0.844, 0.78, 0.4464, 0.3387, 0.1952, 0.1542, 0.1465, 0.037, 0.0119, -0.0263, -0.1485, -0.1515, -2.9375, -2.9428, -3.0061, -3.055, -3.1116, -3.1527, -3.1591, -3.1688, -3.2045, -3.2285, -3.2303, -3.4672, -3.8739, -3.308, -3.3413, -3.5635, -3.8737, -3.5379, -3.3888, -3.5764, 2.3507, 2.136, 1.8391, 1.7014, 1.3979, 1.3407, 1.3283, 1.1505, 1.0539, 1.0083, 0.8225, 0.7888, 0.5745, 0.4329, 0.2995, 0.2615, 0.222, 0.1628, 0.0888, 0.0093, -0.0177, -0.1818, -0.3022, -2.8925, -2.9398, -2.9399, -3.0097, -3.0377, -3.0441, -3.0894, -3.3522, -3.7589, -3.1987, -3.4485, -3.1135, 2.6611, 2.3038, 1.7344, 1.5415, 1.4669, 1.399, 1.2562, 1.2156, 1.0641, 0.8292, 0.8138, 0.3355, 0.206, 0.1704, 0.0191, -0.0361, -0.188, -0.2838, -2.8041, -2.8094, -2.8633, -2.8727, -2.9214, -2.9216, -2.9374, -2.9782, -2.9913, -3.0193, -3.0354, -3.0889, -3.3338, -3.7405, -3.1746, -3.1184, -3.2079, -3.1804, -3.4301, -3.0951, -3.4864, -3.4741, -3.6269, 2.4422, 2.3613, 2.3299, 1.5573, 1.4133, 1.3324, 1.1883, 1.1308, 0.9789, 0.9372, 0.7689, 0.73, 0.5883, 0.4694, 0.3953, 0.0492, -0.062, -0.0748, -0.1961, -0.3433, -2.7859, -2.8451, -2.8546, -2.856, -2.9032, -2.9034, -2.9192, -2.96, -3.0011, -3.0075, -3.3156, -3.7223, -3.0177, -3.0529, -3.1002, -3.1897, -3.1622, -3.7221, -3.3863, -3.2372, 2.4097, 2.3601, 2.0785, 1.6902, 1.6746, 1.3735, 1.2998, 1.153, 1.0145, 0.9768, 0.6243, 0.5842, 0.5623, 0.5106, 0.3828, 0.2442, 0.0877, 0.0187, -0.0947, -0.1501, -0.1526, -0.2578, -2.7464, -2.8584, -2.8586, -2.8743, -2.9152, -2.9283, -2.9563, -2.9627, -3.1115, -3.0081, -3.0553, -3.3671, -3.6773, -3.3414, -3.1924, -3.0592, 2.412, 2.3569, 1.9956, 1.2743, 1.2503, 1.1778, 1.0984, 1.0172, 1.0159, 1.0066, 0.8846, 0.8624, 0.7867, 0.7401, 0.7174, 0.6623, 0.5611, 0.5374, 0.2093, -0.0505, -0.3556, -0.41, -2.7179, -2.7232, -2.7771, -2.7879, -2.892, -2.9051, -2.9492, -2.9497, -3.2421, -3.2476, -3.6543, -3.0884, -2.9849, -3.0321, -3.1217, 2.5927, 2.2857, 1.4029, 1.3962, 1.3707, 1.183, 1.1631, 1.0383, 0.7642, 0.7205, 0.6808, 0.565, 0.4571, 0.4421, 0.378, 0.1697, 0.0594, -0.0185, -0.1006, -0.1405, -1.7148, -2.6213, -2.6847, -2.6861, -2.7335, -2.7493, -2.7901, -2.8033, -2.8312, -2.8473, -3.1457, -3.5524, -2.8479, -2.883, -2.9303, -3.0199, -2.9923, -3.242, -2.907, 2.8476, 2.6658, 2.1488, 1.9574, 1.5179, 1.4734, 1.3193, 1.3191, 1.2655, 0.9043, 0.8589, 0.666, 0.3395, 0.1721, -0.2187, -0.4059, -2.5877, -2.6469, -2.6563, -2.721, -2.7618, -2.7749, -2.8029, -2.8195, -2.8787, -2.8854, -2.902, -2.9043, -2.9058, -2.9083, -3.1174, -3.5241, -2.9582, -2.9915, -2.964, -3.2137, -3.1881, -3.039, 2.3274, 2.1708, 1.6971, 1.543, 1.5187, 1.4536, 1.4078, 1.2885, 1.15, 1.099, 1.0934, 1.0426, 0.5066, 0.3883, 0.3137, 0.2977, 0.1171, -0.1382, -0.3378, -2.4461, -2.4514, -2.5147, -2.5161, -2.5634, -2.5794, -2.6333, -2.6613, -2.6677, -2.6774, -2.6779, -2.8931, -2.9758, -3.3825, -2.8166, -2.7131, -2.7603, -2.8499, -3.0465, -2.8974, -3.085, 2.5422, 2.4363, 2.005, 1.6499, 1.6134, 1.5619, 0.9019, 0.7924, 0.5032, 0.4991, 0.4751, 0.2944, 0.2727, 0.2352, 0.2257, 0.1579, -0.1512, -2.4741, -2.4794, -2.5333, -2.5427, -2.5441, -2.5914, -2.5916, -2.6482, -2.6613, -2.6957, -2.7054, -2.7059, -2.7588, -3.0038, -2.8779, -2.8504, -2.7651, -3.0744, -2.9254, -3.113, -2.7922, -3.1563, -3.077, -3.1441, -3.2969, 3.2621, 2.0802, 1.5543, 1.5415, 1.4425, 1.2662, 1.2474, 0.9822, 0.507, 0.4305, 0.2724, 0.0132, -2.4017, -2.407, -2.4609, -2.4703, -2.4717, -2.519, -2.5192, -2.5758, -2.5889, -2.6169, -2.6233, -2.6329, -2.6335, -2.6687, -2.6864, -2.6927, -2.6944, -2.7159, -2.7779, -2.9307, -2.9314, -3.3381, -2.8055, -3.0277, -3.3379, -3.002, -2.8487, -3.0406, -3.0839, -3.0046, -3.0717, -3.2245, 2.5949, 2.0448, 1.9122, 1.7789, 1.6428, 1.2782, 1.133, 1.0285, 1.0033, 0.8669, 0.7942, 0.64, 0.62, 0.5459, 0.4187, 0.1417, -2.3654, -2.3706, -2.4827, -2.4828, -2.5394, -2.5869, -2.5966, -2.5971, -2.6323, -2.6501, -2.6563, -2.6581, -2.663, -2.6796, -2.895, -3.3017, -2.7358, -2.7692, -2.9913, -3.3015, -2.9657, -2.8166, -2.8124, -3.0043, 2.9364, 2.3666, 2.2495, 2.2373, 2.121, 1.0508, 0.9854, 0.592, 0.5762, 0.5416, 0.4425, 0.0272, -2.335, -2.3403, -2.3942, -2.4037, -2.4051, -2.4523, -2.5091, -2.5222, -2.5502, -2.5566, -2.5663, -2.5668, -2.6198, -2.6493, -2.6516, -2.6532, -2.6556, -2.6647, -2.6684, -2.8641, -2.8163, -2.8647, -3.2714, -2.7055, -2.7388, -2.7113, -2.961, -3.2712, -2.9354, -2.7863, -2.7821, -2.9739, -2.9379, 2.6234, 2.5852, 2.424, 1.9056, 1.5712, 1.0067, 0.9652, 0.7061, 0.598, 0.5086, 0.3485, 0.3323, 0.251, 0.2459, 0.1853, 0.1527, -0.442, -2.3832, -2.4371, -2.4479, -2.4952, -2.4954, -2.5111, -2.552, -2.5651, -2.6091, -2.6097, -2.6449, -2.6626, -2.6689, -2.9076, -3.3143, -2.6921, -2.7817, -2.7541, -3.3141, -2.9782, -2.8249, -3.0168, -3.0601, 2.4014, 1.6997, 1.2105, 1.2027, 1.1637, 1.1621, 1.1579, 1.0458, 1.0372, 0.6931, 0.6392, 0.5946, 0.5354, 0.4119, 0.3751, 0.3631, 0.3414, 0.2235, 0.0036, -2.2181, -2.2234, -2.2773, -2.2867, -2.2882, -2.3354, -2.3356, -2.3922, -2.4333, -2.4397, -2.4494, -2.7478, -2.5886, -2.4851, -2.5324, -2.6219, -2.5944, -2.8441, -3.1543, -2.8185]}, \"token.table\": {\"Topic\": [5, 7, 9, 13, 1, 4, 11, 17, 3, 13, 18, 20, 1, 7, 10, 19, 4, 9, 11, 12, 2, 3, 8, 11, 12, 14, 3, 9, 10, 15, 16, 19, 5, 6, 9, 15, 17, 20, 5, 6, 11, 12, 13, 1, 2, 10, 1, 2, 10, 15, 20, 11, 12, 15, 4, 5, 7, 20, 3, 8, 11, 12, 13, 19, 1, 5, 6, 7, 8, 10, 20, 1, 3, 5, 7, 9, 12, 15, 16, 19, 1, 2, 4, 8, 13, 15, 18, 2, 3, 5, 8, 11, 12, 13, 17, 4, 6, 7, 10, 12, 14, 17, 1, 6, 7, 15, 3, 5, 6, 7, 9, 13, 16, 19, 6, 7, 10, 11, 12, 4, 6, 13, 16, 1, 5, 7, 10, 1, 5, 7, 13, 1, 2, 4, 5, 9, 11, 12, 1, 3, 6, 10, 14, 17, 3, 4, 5, 7, 15, 20, 4, 12, 14, 19, 20, 4, 5, 15, 18, 1, 4, 7, 9, 12, 14, 17, 19, 1, 4, 5, 10, 12, 14, 16, 18, 20, 1, 3, 4, 5, 8, 11, 12, 15, 17, 5, 6, 8, 10, 15, 3, 6, 8, 10, 13, 17, 2, 8, 10, 2, 6, 9, 17, 20, 2, 3, 9, 10, 14, 17, 3, 5, 7, 9, 11, 13, 14, 20, 7, 9, 11, 12, 14, 15, 4, 6, 8, 12, 16, 19, 3, 7, 10, 11, 17, 19, 10, 16, 2, 4, 9, 14, 15, 19, 2, 4, 5, 6, 7, 11, 15, 16, 17, 18, 20, 1, 2, 5, 6, 11, 20, 1, 3, 4, 11, 13, 14, 18, 1, 3, 4, 7, 8, 14, 20, 2, 3, 4, 5, 9, 10, 14, 18, 20, 3, 6, 10, 14, 15, 16, 18, 1, 8, 13, 14, 15, 1, 3, 6, 7, 8, 2, 3, 5, 7, 10, 19, 2, 3, 5, 9, 11, 12, 13, 17, 20, 1, 4, 9, 10, 11, 14, 16, 19, 1, 3, 4, 5, 9, 18, 19, 1, 3, 5, 7, 14, 2, 8, 12, 6, 13, 16, 17, 18, 5, 11, 15, 17, 19, 4, 8, 10, 11, 12, 3, 20, 2, 5, 11, 14, 17, 18, 20, 1, 5, 7, 8, 9, 11, 18, 4, 7, 8, 16, 19, 20, 5, 8, 9, 12, 19, 20, 1, 6, 9, 10, 13, 14], \"Freq\": [0.45648877639745844, 0.1304253646849881, 0.1956380470274822, 0.1956380470274822, 0.335474289780883, 0.0335474289780883, 0.1341897159123532, 0.4696640056932362, 0.39136995780663475, 0.14676373417748803, 0.34244871308080543, 0.09784248945165869, 0.18770037062721573, 0.3003205930035452, 0.03754007412544315, 0.4504808895053178, 0.17511404789713647, 0.13133553592285235, 0.5253421436914094, 0.13133553592285235, 0.17986292846921545, 0.2248286605865193, 0.04496573211730386, 0.04496573211730386, 0.08993146423460772, 0.3597258569384309, 0.08608863511795718, 0.5165318107077431, 0.08608863511795718, 0.04304431755897859, 0.17217727023591436, 0.04304431755897859, 0.060182659560269074, 0.9027398934040362, 0.5613175101064902, 0.0935529183510817, 0.23388229587770426, 0.0935529183510817, 0.23320718730763623, 0.5247161714421815, 0.05830179682690906, 0.05830179682690906, 0.05830179682690906, 0.3860075864634087, 0.5404106210487721, 0.03860075864634087, 0.43692870847265886, 0.12850844366842906, 0.05140337746737163, 0.025701688733685814, 0.3341219535379156, 0.11145306988733494, 0.5944163727324531, 0.2600571630704482, 0.7486099995718467, 0.028792692291224872, 0.17275615374734923, 0.028792692291224872, 0.10505693229583359, 0.15758539844375039, 0.47275619533125113, 0.05252846614791679, 0.10505693229583359, 0.05252846614791679, 0.05199516597820844, 0.10399033195641688, 0.15598549793462532, 0.10399033195641688, 0.20798066391283376, 0.2599758298910422, 0.05199516597820844, 0.18105267100571934, 0.09052633550285967, 0.045263167751429835, 0.045263167751429835, 0.22631583875714917, 0.18105267100571934, 0.045263167751429835, 0.045263167751429835, 0.09052633550285967, 0.1003963856771634, 0.1505945785157451, 0.3011891570314902, 0.1003963856771634, 0.1003963856771634, 0.2007927713543268, 0.0501981928385817, 0.266899193345057, 0.03336239916813213, 0.03336239916813213, 0.06672479833626425, 0.10008719750439639, 0.16681199584066064, 0.03336239916813213, 0.266899193345057, 0.4325330858246803, 0.061790440832097185, 0.061790440832097185, 0.061790440832097185, 0.12358088166419437, 0.12358088166419437, 0.061790440832097185, 0.09576085452367039, 0.23940213630917598, 0.4309238453565168, 0.19152170904734078, 0.48903050853586316, 0.03493075060970451, 0.06986150121940903, 0.06986150121940903, 0.13972300243881805, 0.06986150121940903, 0.03493075060970451, 0.06986150121940903, 0.192206090153519, 0.0768824360614076, 0.5381770524298533, 0.1153236540921114, 0.0384412180307038, 0.04714653113410074, 0.09429306226820149, 0.7071979670115112, 0.09429306226820149, 0.8317960221210494, 0.043778738006371026, 0.043778738006371026, 0.043778738006371026, 0.052023161379776195, 0.10404632275955239, 0.20809264551910478, 0.6242779365573143, 0.038624885886139354, 0.34762397297525416, 0.038624885886139354, 0.23174931531683612, 0.15449954354455742, 0.11587465765841806, 0.038624885886139354, 0.2250063374797646, 0.09000253499190583, 0.045001267495952915, 0.2700076049757175, 0.13500380248785876, 0.18000506998381166, 0.0836349321922179, 0.04181746609610895, 0.1672698643844358, 0.5436270592494163, 0.04181746609610895, 0.0836349321922179, 0.18820231666664003, 0.14115173749998, 0.32935405416662006, 0.23525289583330003, 0.09410115833332001, 0.040770372590644297, 0.24462223554386578, 0.040770372590644297, 0.6523259614503087, 0.04051329748226961, 0.4456462723049657, 0.20256648741134803, 0.04051329748226961, 0.08102659496453922, 0.04051329748226961, 0.08102659496453922, 0.04051329748226961, 0.041310376782454555, 0.16524150712981822, 0.16524150712981822, 0.08262075356490911, 0.041310376782454555, 0.2065518839122728, 0.12393113034736368, 0.08262075356490911, 0.041310376782454555, 0.2192409885628031, 0.04384819771256062, 0.13154459313768185, 0.17539279085024248, 0.04384819771256062, 0.17539279085024248, 0.08769639542512124, 0.04384819771256062, 0.04384819771256062, 0.16770658405462988, 0.1257799380409724, 0.5031197521638896, 0.08385329202731494, 0.08385329202731494, 0.2445002032906956, 0.0611250508226739, 0.0611250508226739, 0.1833751524680217, 0.3056252541133695, 0.1222501016453478, 0.5537040183639509, 0.27685200918197544, 0.13842600459098772, 0.5436632724840833, 0.05436632724840833, 0.2174653089936333, 0.05436632724840833, 0.05436632724840833, 0.5859358982667898, 0.027901709441275704, 0.08370512832382711, 0.08370512832382711, 0.16741025664765422, 0.027901709441275704, 0.12438004785821967, 0.20730007976369946, 0.08292003190547978, 0.08292003190547978, 0.08292003190547978, 0.16584006381095956, 0.16584006381095956, 0.08292003190547978, 0.3452464301158986, 0.09415811730433599, 0.03138603910144533, 0.4394045474202346, 0.03138603910144533, 0.06277207820289066, 0.04769471985329648, 0.5723366382395577, 0.19077887941318591, 0.04769471985329648, 0.04769471985329648, 0.04769471985329648, 0.18363248141577215, 0.12242165427718144, 0.3672649628315443, 0.12242165427718144, 0.06121082713859072, 0.06121082713859072, 0.033819270822317594, 0.9469395830248926, 0.21033785262630236, 0.03505630877105039, 0.03505630877105039, 0.17528154385525196, 0.4557320140236551, 0.03505630877105039, 0.11476094205466623, 0.05738047102733312, 0.05738047102733312, 0.05738047102733312, 0.05738047102733312, 0.11476094205466623, 0.05738047102733312, 0.28690235513666557, 0.05738047102733312, 0.05738047102733312, 0.05738047102733312, 0.16146039603034704, 0.08073019801517352, 0.08073019801517352, 0.24219059404552057, 0.36328589106828085, 0.04036509900758676, 0.1165856692700978, 0.1165856692700978, 0.2914641731752445, 0.0582928346350489, 0.1748785039051467, 0.1165856692700978, 0.0582928346350489, 0.23288362970470664, 0.34932544455706, 0.1552557531364711, 0.03881393828411778, 0.03881393828411778, 0.1552557531364711, 0.03881393828411778, 0.04900668148855, 0.04900668148855, 0.04900668148855, 0.04900668148855, 0.04900668148855, 0.53907349637405, 0.04900668148855, 0.04900668148855, 0.04900668148855, 0.14604260312227882, 0.048680867707426266, 0.14604260312227882, 0.048680867707426266, 0.048680867707426266, 0.14604260312227882, 0.3407660739519839, 0.694094254679554, 0.051414389235522516, 0.025707194617761258, 0.025707194617761258, 0.17995036232432882, 0.23884528872607663, 0.09553811549043066, 0.3343834042165073, 0.09553811549043066, 0.19107623098086132, 0.06555846114997867, 0.196675383449936, 0.393350766899872, 0.06555846114997867, 0.06555846114997867, 0.13111692229995733, 0.18790078618991923, 0.04697519654747981, 0.09395039309495962, 0.04697519654747981, 0.09395039309495962, 0.09395039309495962, 0.18790078618991923, 0.09395039309495962, 0.09395039309495962, 0.09515721611344762, 0.04757860805672381, 0.04757860805672381, 0.14273582417017142, 0.09515721611344762, 0.04757860805672381, 0.04757860805672381, 0.4282074725105142, 0.2411944057171009, 0.16079627047806727, 0.08039813523903364, 0.16079627047806727, 0.2009953380975841, 0.08039813523903364, 0.04019906761951682, 0.11016931348215409, 0.11016931348215409, 0.44067725392861634, 0.22033862696430817, 0.11016931348215409, 0.14386742925841767, 0.7553040036066928, 0.07193371462920883, 0.04643491168899496, 0.3714792935119597, 0.1393047350669849, 0.09286982337798992, 0.32504438182296475, 0.2114627896179152, 0.1585970922134364, 0.4229255792358304, 0.1057313948089576, 0.1057313948089576, 0.5069054859615418, 0.042242123830128486, 0.2112106191506424, 0.16896849532051395, 0.08448424766025697, 0.8190984371987463, 0.16381968743974926, 0.49708667917165916, 0.033139111944777276, 0.033139111944777276, 0.09941733583433182, 0.1656955597238864, 0.033139111944777276, 0.09941733583433182, 0.22481658207512728, 0.044963316415025455, 0.044963316415025455, 0.13488994924507636, 0.08992663283005091, 0.13488994924507636, 0.2697798984901527, 0.0834967388703423, 0.12524510830551344, 0.1669934777406846, 0.1669934777406846, 0.3757353249165403, 0.04174836943517115, 0.35876471221274797, 0.2511352985489236, 0.10762941366382439, 0.17938235610637399, 0.0358764712212748, 0.0358764712212748, 0.1479436361911574, 0.049314545397052476, 0.5917745447646297, 0.049314545397052476, 0.049314545397052476, 0.049314545397052476], \"Term\": [\"appli\", \"appli\", \"appli\", \"appli\", \"approxim\", \"approxim\", \"approxim\", \"approxim\", \"avail\", \"avail\", \"avail\", \"avail\", \"bound\", \"bound\", \"bound\", \"bound\", \"calcul\", \"calcul\", \"calcul\", \"calcul\", \"challeng\", \"challeng\", \"challeng\", \"challeng\", \"challeng\", \"challeng\", \"chang\", \"chang\", \"chang\", \"chang\", \"chang\", \"chang\", \"class\", \"class\", \"control\", \"control\", \"control\", \"control\", \"correspond\", \"correspond\", \"correspond\", \"correspond\", \"correspond\", \"dataset\", \"dataset\", \"dataset\", \"detect\", \"detect\", \"detect\", \"detect\", \"detect\", \"electron\", \"electron\", \"electron\", \"equat\", \"equat\", \"equat\", \"equat\", \"establish\", \"establish\", \"establish\", \"establish\", \"establish\", \"establish\", \"exampl\", \"exampl\", \"exampl\", \"exampl\", \"exampl\", \"exampl\", \"exampl\", \"exist\", \"exist\", \"exist\", \"exist\", \"exist\", \"exist\", \"exist\", \"exist\", \"exist\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"featur\", \"featur\", \"featur\", \"featur\", \"featur\", \"featur\", \"featur\", \"featur\", \"final\", \"final\", \"final\", \"final\", \"final\", \"final\", \"final\", \"finit\", \"finit\", \"finit\", \"finit\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"group\", \"group\", \"group\", \"group\", \"group\", \"identifi\", \"identifi\", \"identifi\", \"identifi\", \"imag\", \"imag\", \"imag\", \"imag\", \"implement\", \"implement\", \"implement\", \"implement\", \"import\", \"import\", \"import\", \"import\", \"import\", \"import\", \"import\", \"improv\", \"improv\", \"improv\", \"improv\", \"improv\", \"improv\", \"interact\", \"interact\", \"interact\", \"interact\", \"interact\", \"interact\", \"investig\", \"investig\", \"investig\", \"investig\", \"investig\", \"known\", \"known\", \"known\", \"known\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"multipl\", \"multipl\", \"multipl\", \"multipl\", \"multipl\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"neural\", \"neural\", \"neural\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"oper\", \"oper\", \"oper\", \"oper\", \"oper\", \"oper\", \"oper\", \"oper\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"posit\", \"posit\", \"posit\", \"posit\", \"posit\", \"posit\", \"possibl\", \"possibl\", \"possibl\", \"possibl\", \"possibl\", \"possibl\", \"power\", \"power\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"rang\", \"rang\", \"rang\", \"rang\", \"rang\", \"rang\", \"rang\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"reduc\", \"reduc\", \"reduc\", \"reduc\", \"reduc\", \"reduc\", \"reduc\", \"reduc\", \"reduc\", \"requir\", \"requir\", \"requir\", \"requir\", \"requir\", \"requir\", \"requir\", \"sampl\", \"sampl\", \"sampl\", \"sampl\", \"sampl\", \"set\", \"set\", \"set\", \"set\", \"set\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"signific\", \"signific\", \"signific\", \"signific\", \"signific\", \"signific\", \"signific\", \"signific\", \"signific\", \"simpl\", \"simpl\", \"simpl\", \"simpl\", \"simpl\", \"simpl\", \"simpl\", \"simpl\", \"solv\", \"solv\", \"solv\", \"solv\", \"solv\", \"solv\", \"solv\", \"special\", \"special\", \"special\", \"special\", \"special\", \"specif\", \"specif\", \"specif\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"surfac\", \"surfac\", \"surfac\", \"surfac\", \"surfac\", \"system\", \"system\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"techniqu\", \"techniqu\", \"techniqu\", \"techniqu\", \"techniqu\", \"techniqu\", \"techniqu\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"valu\", \"valu\", \"valu\", \"valu\", \"valu\", \"valu\", \"variabl\", \"variabl\", \"variabl\", \"variabl\", \"variabl\", \"variabl\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [17, 7, 18, 19, 15, 5, 4, 2, 20, 6, 14, 16, 1, 10, 12, 8, 9, 13, 11, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el881219972227725441038353824\", ldavis_el881219972227725441038353824_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el881219972227725441038353824\", ldavis_el881219972227725441038353824_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el881219972227725441038353824\", ldavis_el881219972227725441038353824_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster      Freq\n",
       "topic                                               \n",
       "16    -0.053997 -0.008641       1        1  9.038942\n",
       "6     -0.272353 -0.166163       2        1  7.826609\n",
       "17     0.125250 -0.146284       3        1  7.207336\n",
       "18     0.004622  0.084935       4        1  6.219633\n",
       "14     0.032395  0.039444       5        1  5.894165\n",
       "4      0.092965  0.098287       6        1  5.869207\n",
       "3      0.118070  0.196378       7        1  5.264933\n",
       "1     -0.066603  0.123662       8        1  5.171105\n",
       "19    -0.021940 -0.008867       9        1  5.154705\n",
       "5      0.002463 -0.041937      10        1  4.947907\n",
       "13    -0.142647  0.066664      11        1  4.698770\n",
       "15    -0.155732  0.194722      12        1  4.385037\n",
       "0      0.149117 -0.107034      13        1  4.208922\n",
       "9     -0.064021 -0.108419      14        1  3.716484\n",
       "11    -0.085278  0.108036      15        1  3.630608\n",
       "7      0.233636 -0.019387      16        1  3.572003\n",
       "8     -0.168264 -0.125301      17        1  3.412261\n",
       "12     0.162522 -0.182408      18        1  3.388646\n",
       "10     0.114294  0.150846      19        1  3.346272\n",
       "2     -0.004498 -0.148534      20        1  3.046454, topic_info=       Term       Freq      Total Category  logprob  loglift\n",
       "56   system  48.000000  48.000000  Default  30.0000  30.0000\n",
       "65    power  29.000000  29.000000  Default  29.0000  29.0000\n",
       "52    class  33.000000  33.000000  Default  28.0000  28.0000\n",
       "1    detect  38.000000  38.000000  Default  27.0000  27.0000\n",
       "19    equat  34.000000  34.000000  Default  26.0000  26.0000\n",
       "..      ...        ...        ...      ...      ...      ...\n",
       "6      imag   0.050564  22.842139  Topic20  -6.8958  -2.6219\n",
       "7    improv   0.050564  22.221596  Topic20  -6.8958  -2.5944\n",
       "8   predict   0.050564  28.525536  Topic20  -6.8958  -2.8441\n",
       "10    sampl   0.050564  38.899616  Topic20  -6.8958  -3.1543\n",
       "11   specif   0.050564  27.803374  Topic20  -6.8958  -2.8185\n",
       "\n",
       "[808 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "22        5  0.456489     appli\n",
       "22        7  0.130425     appli\n",
       "22        9  0.195638     appli\n",
       "22       13  0.195638     appli\n",
       "18        1  0.335474  approxim\n",
       "...     ...       ...       ...\n",
       "33        6  0.049315   variabl\n",
       "33        9  0.591775   variabl\n",
       "33       10  0.049315   variabl\n",
       "33       13  0.049315   variabl\n",
       "33       14  0.049315   variabl\n",
       "\n",
       "[388 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[17, 7, 18, 19, 15, 5, 4, 2, 20, 6, 14, 16, 1, 10, 12, 8, 9, 13, 11, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YouTrend",
   "language": "python",
   "name": "youtrend"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
